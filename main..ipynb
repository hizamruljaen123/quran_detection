{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e94266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Custom RNN Cell\n",
    "class CustomRNNCell(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(CustomRNNCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                    initializer='glorot_uniform',\n",
    "                                    name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='orthogonal',\n",
    "            name='recurrent_kernel')\n",
    "        self.bias = self.add_weight(shape=(self.units,),\n",
    "                                  initializer='zeros',\n",
    "                                  name='bias')\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.matmul(inputs, self.kernel)\n",
    "        output = tf.nn.tanh(h + tf.matmul(prev_output, self.recurrent_kernel) + self.bias)\n",
    "        return output, [output]\n",
    "\n",
    "# Custom Optimizer\n",
    "class CustomAdam(tf.keras.optimizers.Adam):\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, **kwargs):\n",
    "        super(CustomAdam, self).__init__(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            epsilon=epsilon,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Data preprocessing function\n",
    "def extract_features(file_path, max_length=128):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        # Extract spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)\n",
    "        \n",
    "        # Combine features\n",
    "        features = np.vstack([mfccs, spectral_centroids, spectral_rolloff, zero_crossing_rate])\n",
    "        \n",
    "        # Pad or truncate to fixed length\n",
    "        if features.shape[1] < max_length:\n",
    "            features = np.pad(features, ((0, 0), (0, max_length - features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            features = features[:, :max_length]\n",
    "            \n",
    "        return features.T  # Shape: (max_length, n_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data from folders - Surah An-Naba Dataset\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load audio data dari multiple pembaca Quran (Surah An-Naba)\n",
    "    \n",
    "    Dataset Structure:\n",
    "    - sample_1/, sample_2/, ..., sample_7/ = 7 pembaca berbeda\n",
    "    - Setiap folder berisi 41 files: 078000.mp3 sampai 078040.mp3\n",
    "    - 078000.mp3 = Bismillah (ayat pembuka) -> label 0\n",
    "    - 078001.mp3 = Ayat 1 -> label 1\n",
    "    - ...\n",
    "    - 078040.mp3 = Ayat 40 -> label 40\n",
    "    \n",
    "    Total: 7 pembaca √ó 41 ayat = 287 audio samples\n",
    "    \"\"\"\n",
    "    base_path = r\"d:\\new_project\\quran_detect\"\n",
    "    \n",
    "    # Cek semua folder sample yang tersedia (sample_1 hingga sample_7)\n",
    "    available_folders = []\n",
    "    for i in range(1, 8):  # sample_1 sampai sample_7\n",
    "        folder_name = f'sample_{i}'\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            available_folders.append(folder_name)\n",
    "        else:\n",
    "            print(f\"Warning: Folder {folder_name} not found\")\n",
    "    \n",
    "    print(f\"Found {len(available_folders)} sample folders: {available_folders}\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    file_info = []  # Track file information for debugging\n",
    "    \n",
    "    for folder in available_folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        print(f\"Processing {folder}...\")\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(folder_path) if f.endswith('.mp3')])\n",
    "        print(f\"  Found {len(files)} audio files\")\n",
    "        \n",
    "        # Validasi bahwa file sesuai format yang diharapkan\n",
    "        expected_files = [f\"078{i:03d}.mp3\" for i in range(41)]  # 078000.mp3 to 078040.mp3\n",
    "        missing_files = [f for f in expected_files if f not in files]\n",
    "        if missing_files:\n",
    "            print(f\"  Warning: Missing files in {folder}: {missing_files[:5]}...\")\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {folder}\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            \n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                \n",
    "                # Extract verse number from filename\n",
    "                # 078000.mp3 -> 0 (Bismillah)\n",
    "                # 078001.mp3 -> 1 (Ayat 1)\n",
    "                # 078040.mp3 -> 40 (Ayat 40)\n",
    "                verse_num = int(file.split('.')[0][-3:])\n",
    "                y.append(verse_num)\n",
    "                \n",
    "                # Track file info for debugging\n",
    "                file_info.append({\n",
    "                    'folder': folder,\n",
    "                    'filename': file,\n",
    "                    'verse_label': verse_num\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  Failed to extract features from: {file}\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Print dataset summary\n",
    "    print(f\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Total pembaca: {len(available_folders)}\")\n",
    "    print(f\"Feature shape per sample: {X[0].shape if len(X) > 0 else 'N/A'}\")\n",
    "    print(f\"Verse range: {min(y)} to {max(y)}\")\n",
    "    print(f\"Unique verses: {len(np.unique(y))}\")\n",
    "    \n",
    "    # Cek distribusi ayat\n",
    "    verse_counts = np.bincount(y)\n",
    "    print(f\"Samples per verse (should be {len(available_folders)} each):\")\n",
    "    for verse_id in range(len(verse_counts)):\n",
    "        if verse_counts[verse_id] > 0:\n",
    "            verse_name = \"Bismillah\" if verse_id == 0 else f\"Ayat {verse_id}\"\n",
    "            print(f\"  {verse_name} (ID:{verse_id}): {verse_counts[verse_id]} samples\")\n",
    "    \n",
    "    return X, y, file_info\n",
    "\n",
    "# Custom RNN Model\n",
    "def create_custom_rnn_model(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Custom RNN layers\n",
    "    rnn_cell1 = CustomRNNCell(128)\n",
    "    rnn_layer1 = layers.RNN(rnn_cell1, return_sequences=True)(inputs)\n",
    "    rnn_layer1 = layers.Dropout(0.3)(rnn_layer1)\n",
    "    \n",
    "    rnn_cell2 = CustomRNNCell(64)\n",
    "    rnn_layer2 = layers.RNN(rnn_cell2, return_sequences=False)(rnn_layer1)\n",
    "    rnn_layer2 = layers.Dropout(0.3)(rnn_layer2)\n",
    "    \n",
    "    # Dense layers\n",
    "    dense1 = layers.Dense(256, activation='relu')(rnn_layer2)\n",
    "    dense1 = layers.Dropout(0.4)(dense1)\n",
    "    dense2 = layers.Dense(128, activation='relu')(dense1)\n",
    "    dense2 = layers.Dropout(0.3)(dense2)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(dense2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Function to save model and metadata\n",
    "def save_model_and_metadata(model, label_encoder, history, model_dir=\"model_saves\"):\n",
    "    \"\"\"\n",
    "    Simpan model, label encoder, dan metadata training\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, \"quran_verse_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"Label encoder saved to: {encoder_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(model_dir, \"training_history.pkl\")\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"num_classes\": len(label_encoder.classes_),\n",
    "        \"verse_labels\": label_encoder.classes_.tolist(),\n",
    "        \"input_shape\": model.input_shape[1:],  # Remove batch dimension\n",
    "        \"total_epochs\": len(history.history['loss']),\n",
    "        \"final_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"final_loss\": history.history['val_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"model_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Function to load model and metadata\n",
    "def load_model_and_metadata(model_dir=\"model_saves\"):\n",
    "    \"\"\"\n",
    "    Muat model, label encoder, dan metadata yang telah disimpan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model\n",
    "        model_path = os.path.join(model_dir, \"quran_verse_model.h5\")\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects={\n",
    "            'CustomRNNCell': CustomRNNCell,\n",
    "            'CustomAdam': CustomAdam\n",
    "        })\n",
    "        print(f\"Model loaded from: {model_path}\")\n",
    "        \n",
    "        # Load label encoder\n",
    "        encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        print(f\"Label encoder loaded from: {encoder_path}\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(model_dir, \"model_metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"Metadata loaded from: {metadata_path}\")\n",
    "        \n",
    "        # Load training history if needed\n",
    "        history_path = os.path.join(model_dir, \"training_history.pkl\")\n",
    "        history = None\n",
    "        if os.path.exists(history_path):\n",
    "            with open(history_path, 'rb') as f:\n",
    "                history = pickle.load(f)\n",
    "            print(f\"Training history loaded from: {history_path}\")\n",
    "        \n",
    "        return model, label_encoder, metadata, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Function to predict single audio file\n",
    "def predict_verse(model, label_encoder, audio_file_path):\n",
    "    \"\"\"\n",
    "    Prediksi nomor ayat dari file audio tunggal\n",
    "    \n",
    "    Returns:\n",
    "        verse_number (int): Nomor ayat (0=Bismillah, 1-40=Ayat 1-40)\n",
    "        confidence (float): Confidence score (0-1)\n",
    "    \"\"\"\n",
    "    # Extract features from audio file\n",
    "    features = extract_features(audio_file_path)\n",
    "    if features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    features = features.reshape(1, features.shape[0], features.shape[1])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features, verbose=0)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Convert back to original label\n",
    "    verse_number = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return verse_number, confidence\n",
    "\n",
    "def get_verse_name(verse_number):\n",
    "    \"\"\"\n",
    "    Konversi nomor ayat ke nama yang mudah dibaca\n",
    "    \n",
    "    Args:\n",
    "        verse_number (int): 0=Bismillah, 1-40=Ayat 1-40\n",
    "    \n",
    "    Returns:\n",
    "        str: Nama ayat yang mudah dibaca\n",
    "    \"\"\"\n",
    "    if verse_number == 0:\n",
    "        return \"Bismillah (Pembuka)\"\n",
    "    elif 1 <= verse_number <= 40:\n",
    "        return f\"Ayat {verse_number}\"\n",
    "    else:\n",
    "        return f\"Unknown ({verse_number})\"\n",
    "\n",
    "def predict_verse_with_details(model, label_encoder, audio_file_path):\n",
    "    \"\"\"\n",
    "    Prediksi dengan output yang lebih detail dan mudah dibaca\n",
    "    \"\"\"\n",
    "    verse_number, confidence = predict_verse(model, label_encoder, audio_file_path)\n",
    "    \n",
    "    if verse_number is None:\n",
    "        return None\n",
    "    \n",
    "    verse_name = get_verse_name(verse_number)\n",
    "    \n",
    "    result = {\n",
    "        'verse_number': verse_number,\n",
    "        'verse_name': verse_name,\n",
    "        'confidence': confidence,\n",
    "        'confidence_percent': confidence * 100,\n",
    "        'file_path': audio_file_path,\n",
    "        'filename': os.path.basename(audio_file_path)\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to test on a folder of audio files\n",
    "def test_on_folder(model, label_encoder, test_folder_path):\n",
    "    \"\"\"\n",
    "    Test model pada folder berisi file audio dan tampilkan hasil\n",
    "    \"\"\"\n",
    "    if not os.path.exists(test_folder_path):\n",
    "        print(f\"Test folder not found: {test_folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    files = sorted([f for f in os.listdir(test_folder_path) if f.endswith('.mp3')])\n",
    "    \n",
    "    print(f\"Testing on {len(files)} files from {test_folder_path}\")\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        file_path = os.path.join(test_folder_path, file)\n",
    "        \n",
    "        # Get actual verse number from filename\n",
    "        actual_verse = int(file.split('.')[0][-3:])\n",
    "        \n",
    "        # Predict\n",
    "        predicted_verse, confidence = predict_verse(model, label_encoder, file_path)\n",
    "        \n",
    "        if predicted_verse is not None:\n",
    "            # Get verse names for better readability\n",
    "            actual_verse_name = get_verse_name(actual_verse)\n",
    "            predicted_verse_name = get_verse_name(predicted_verse)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': file,\n",
    "                'actual_verse': actual_verse,\n",
    "                'actual_verse_name': actual_verse_name,\n",
    "                'predicted_verse': predicted_verse,\n",
    "                'predicted_verse_name': predicted_verse_name,\n",
    "                'confidence': confidence,\n",
    "                'correct': actual_verse == predicted_verse\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if results:\n",
    "        accuracy = sum([r['correct'] for r in results]) / len(results)\n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Total files tested: {len(results)}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"Correct predictions: {sum([r['correct'] for r in results])}\")\n",
    "        print(f\"Wrong predictions: {sum([not r['correct'] for r in results])}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(f\"\\nFirst 10 predictions:\")\n",
    "        for i, result in enumerate(results[:10]):\n",
    "            status = \"‚úì\" if result['correct'] else \"‚úó\"\n",
    "            print(f\"{status} {result['filename']}: {result['actual_verse_name']} -> {result['predicted_verse_name']} (conf: {result['confidence']:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Training function with history tracking and model saving\n",
    "def train_model(X, y, save_model=True):\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Convert to categorical\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "    \n",
    "    # Check if we have enough samples for stratified split\n",
    "    total_samples = len(X)\n",
    "    test_size = 0.2\n",
    "    min_test_samples = int(total_samples * test_size)\n",
    "    \n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Minimum test samples needed: {min_test_samples}\")\n",
    "    \n",
    "    # Split data - use stratify only if we have enough samples\n",
    "    if min_test_samples >= num_classes and total_samples >= num_classes * 2:\n",
    "        print(\"Using stratified split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_categorical, test_size=test_size, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "    else:\n",
    "        print(\"Dataset too small for stratified split, using random split...\")\n",
    "        # Use smaller test size or random split without stratification\n",
    "        test_size = max(0.1, min_test_samples / total_samples) if total_samples < 100 else 0.2\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_categorical, test_size=test_size, random_state=42, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_custom_rnn_model((X.shape[1], X.shape[2]), num_classes)\n",
    "    \n",
    "    # Custom optimizer\n",
    "    custom_optimizer = CustomAdam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=custom_optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Adjust training parameters based on dataset size\n",
    "    total_train_samples = len(X_train)\n",
    "    \n",
    "    # Adjust batch size for small datasets\n",
    "    if total_train_samples < 100:\n",
    "        batch_size = min(8, total_train_samples // 4)  # Very small batch for tiny datasets\n",
    "        epochs = 50  # Fewer epochs for small datasets\n",
    "        patience = 5\n",
    "    elif total_train_samples < 500:\n",
    "        batch_size = min(16, total_train_samples // 8)\n",
    "        epochs = 75\n",
    "        patience = 7\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        epochs = 100\n",
    "        patience = 10\n",
    "    \n",
    "    print(f\"Training with batch_size={batch_size}, epochs={epochs}, patience={patience}\")\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=patience, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=max(2, patience//2), min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model and metadata if requested\n",
    "    if save_model:\n",
    "        save_model_and_metadata(model, label_encoder, history)\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "# Plotting function using Plotly\n",
    "def plot_training_history(history):\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Training & Validation Loss', 'Training & Validation Accuracy',\n",
    "                       'Training & Validation Precision', 'Training & Validation Recall'),\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['loss'], \n",
    "                  mode='lines', name='Training Loss', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_loss'], \n",
    "                  mode='lines', name='Validation Loss', line=dict(color='orange')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['accuracy'], \n",
    "                  mode='lines', name='Training Accuracy', line=dict(color='blue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_accuracy'], \n",
    "                  mode='lines', name='Validation Accuracy', line=dict(color='lightblue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Precision plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['precision'], \n",
    "                  mode='lines', name='Training Precision', line=dict(color='green')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_precision'], \n",
    "                  mode='lines', name='Validation Precision', line=dict(color='lightgreen')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Recall plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['recall'], \n",
    "                  mode='lines', name='Training Recall', line=dict(color='purple')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_recall'], \n",
    "                  mode='lines', name='Validation Recall', line=dict(color='plum')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Training Progress - Quran Verse Detection (An-Naba)',\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Epochs\")\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Precision\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Recall\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Main execution - Quran Verse Detection Training\n",
    "print(\"=== QURAN VERSE DETECTION - SURAH AN-NABA ===\")\n",
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# Load data with file tracking information\n",
    "X, y, file_info = load_data()\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Number of verses (classes): {len(np.unique(y))}\")\n",
    "print(f\"Verse range: {min(y)} to {max(y)}\")\n",
    "\n",
    "# Print beberapa contoh data untuk verifikasi\n",
    "print(f\"\\nSample data verification:\")\n",
    "for i in range(min(5, len(file_info))):\n",
    "    info = file_info[i]\n",
    "    verse_name = \"Bismillah\" if info['verse_label'] == 0 else f\"Ayat {info['verse_label']}\"\n",
    "    print(f\"  {info['folder']}/{info['filename']} -> {verse_name} (label: {info['verse_label']})\")\n",
    "\n",
    "# Cek apakah dataset cukup untuk training\n",
    "if len(X) < 10:\n",
    "    print(f\"\\nWarning: Dataset terlalu kecil ({len(X)} samples). Minimal 10 samples diperlukan.\")\n",
    "    print(\"Pastikan folder sample_1 hingga sample_7 tersedia dan berisi file audio yang valid.\")\n",
    "else:\n",
    "    print(f\"\\nDataset ready for training: {len(X)} samples from {len(np.unique(y))} verses\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "model, history, label_encoder = train_model(X, y, save_model=True)\n",
    "\n",
    "print(\"\\nGenerating training plots...\")\n",
    "training_plot = plot_training_history(history)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"Model dan hasil training telah selesai. Grafik menunjukkan progress training untuk deteksi ayat Al-Quran Surah An-Naba.\")\n",
    "print(\"Model, label encoder, dan metadata telah disimpan di folder 'model_saves'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= CONTOH PENGGUNAAN MODEL YANG TELAH DISIMPAN =======\n",
    "# Jalankan code ini untuk menggunakan model yang sudah dilatih sebelumnya\n",
    "\n",
    "# 1. Load model yang telah disimpan\n",
    "print(\"Loading saved model...\")\n",
    "saved_model, saved_label_encoder, metadata, saved_history = load_model_and_metadata(\"model_saves\")\n",
    "\n",
    "if saved_model is not None:\n",
    "    print(\"\\n=== Model Information ===\")\n",
    "    print(f\"Number of classes: {metadata['num_classes']}\")\n",
    "    print(f\"Verse labels: {metadata['verse_labels'][:10]}...\")  # Show first 10\n",
    "    print(f\"Input shape: {metadata['input_shape']}\")\n",
    "    print(f\"Final training accuracy: {metadata['final_accuracy']:.4f}\")\n",
    "    print(f\"Final training loss: {metadata['final_loss']:.4f}\")\n",
    "    \n",
    "    # 2. Test pada single file\n",
    "    print(\"\\n=== Testing Single File ===\")\n",
    "    # Contoh test pada file dari sample_1\n",
    "    test_file = r\"d:\\new_project\\quran_detect\\sample_1\\078000.mp3\"\n",
    "    if os.path.exists(test_file):\n",
    "        predicted_verse, confidence = predict_verse(saved_model, saved_label_encoder, test_file)\n",
    "        print(f\"File: {test_file}\")\n",
    "        print(f\"Predicted verse: {predicted_verse}\")\n",
    "        print(f\"Confidence: {confidence:.4f}\")\n",
    "        \n",
    "        # Get actual verse from filename\n",
    "        actual_verse = int(os.path.basename(test_file).split('.')[0][-3:])\n",
    "        print(f\"Actual verse: {actual_verse}\")\n",
    "        print(f\"Prediction correct: {predicted_verse == actual_verse}\")\n",
    "    \n",
    "    # 3. Test pada seluruh folder (gunakan salah satu folder yang belum digunakan untuk training)\n",
    "    print(\"\\n=== Testing on Folder ===\")\n",
    "    # Misalnya test pada sample_1 (atau buat folder test baru)\n",
    "    test_folder = r\"d:\\new_project\\quran_detect\\sample_1\"\n",
    "    if os.path.exists(test_folder):\n",
    "        test_results = test_on_folder(saved_model, saved_label_encoder, test_folder)\n",
    "        \n",
    "        if test_results:\n",
    "            # Analisis hasil test\n",
    "            correct_predictions = [r for r in test_results if r['correct']]\n",
    "            wrong_predictions = [r for r in test_results if not r['correct']]\n",
    "            \n",
    "            print(f\"\\n=== Detailed Analysis ===\")\n",
    "            if wrong_predictions:\n",
    "                print(\"Wrong predictions:\")\n",
    "                for wrong in wrong_predictions[:5]:  # Show first 5 wrong predictions\n",
    "                    print(f\"  {wrong['filename']}: {wrong['actual_verse']} -> {wrong['predicted_verse']} (conf: {wrong['confidence']:.3f})\")\n",
    "            \n",
    "            # Average confidence\n",
    "            avg_confidence = np.mean([r['confidence'] for r in test_results])\n",
    "            print(f\"Average confidence: {avg_confidence:.4f}\")\n",
    "            \n",
    "            # Confidence for correct vs wrong predictions\n",
    "            if correct_predictions:\n",
    "                avg_correct_conf = np.mean([r['confidence'] for r in correct_predictions])\n",
    "                print(f\"Average confidence for correct predictions: {avg_correct_conf:.4f}\")\n",
    "            \n",
    "            if wrong_predictions:\n",
    "                avg_wrong_conf = np.mean([r['confidence'] for r in wrong_predictions])\n",
    "                print(f\"Average confidence for wrong predictions: {avg_wrong_conf:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No saved model found. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5997374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= UTILITY FUNCTIONS UNTUK EVALUASI DAN TESTING =======\n",
    "\n",
    "def evaluate_model_performance(model, label_encoder, test_folders):\n",
    "    \"\"\"\n",
    "    Evaluasi performa model pada multiple test folders\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for folder_name in test_folders:\n",
    "        folder_path = os.path.join(r\"d:\\new_project\\quran_detect\", folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            print(f\"\\nEvaluating on {folder_name}...\")\n",
    "            results = test_on_folder(model, label_encoder, folder_path)\n",
    "            all_results[folder_name] = results\n",
    "        else:\n",
    "            print(f\"Folder {folder_path} not found, skipping...\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def create_confusion_matrix_data(test_results):\n",
    "    \"\"\"\n",
    "    Buat data untuk confusion matrix dari hasil test\n",
    "    \"\"\"\n",
    "    actual_verses = [r['actual_verse'] for r in test_results]\n",
    "    predicted_verses = [r['predicted_verse'] for r in test_results]\n",
    "    \n",
    "    # Get unique verses\n",
    "    unique_verses = sorted(list(set(actual_verses + predicted_verses)))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    confusion_matrix = np.zeros((len(unique_verses), len(unique_verses)))\n",
    "    \n",
    "    for actual, predicted in zip(actual_verses, predicted_verses):\n",
    "        actual_idx = unique_verses.index(actual)\n",
    "        predicted_idx = unique_verses.index(predicted)\n",
    "        confusion_matrix[actual_idx][predicted_idx] += 1\n",
    "    \n",
    "    return confusion_matrix, unique_verses\n",
    "\n",
    "def predict_batch_files(model, label_encoder, file_paths):\n",
    "    \"\"\"\n",
    "    Prediksi batch files sekaligus\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(file_paths)} files...\")\n",
    "    for file_path in tqdm(file_paths):\n",
    "        if os.path.exists(file_path):\n",
    "            predicted_verse, confidence = predict_verse(model, label_encoder, file_path)\n",
    "            results.append({\n",
    "                'file_path': file_path,\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'predicted_verse': predicted_verse,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_predictions_to_csv(predictions, output_file=\"predictions.csv\"):\n",
    "    \"\"\"\n",
    "    Simpan hasil prediksi ke CSV file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(predictions)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to: {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Contoh penggunaan batch prediction\n",
    "print(\"=== BATCH PREDICTION EXAMPLE ===\")\n",
    "print(\"Uncomment the code below to run batch prediction:\")\n",
    "\n",
    "# # Contoh untuk predict multiple files\n",
    "# file_list = [\n",
    "#     r\"d:\\new_project\\quran_detect\\sample_1\\078000.mp3\",\n",
    "#     r\"d:\\new_project\\quran_detect\\sample_1\\078001.mp3\",\n",
    "#     r\"d:\\new_project\\quran_detect\\sample_1\\078002.mp3\",\n",
    "#     # Tambahkan file lain...\n",
    "# ]\n",
    "\n",
    "# if saved_model is not None:\n",
    "#     batch_results = predict_batch_files(saved_model, saved_label_encoder, file_list)\n",
    "#     predictions_df = save_predictions_to_csv(batch_results, \"batch_predictions.csv\")\n",
    "#     print(predictions_df.head())\n",
    "\n",
    "print(\"\\nModel telah siap untuk digunakan!\")\n",
    "print(\"File yang tersimpan:\")\n",
    "print(\"- model_saves/quran_verse_model.h5 (Model TensorFlow)\")\n",
    "print(\"- model_saves/label_encoder.pkl (Label Encoder)\")\n",
    "print(\"- model_saves/model_metadata.json (Metadata)\")\n",
    "print(\"- model_saves/training_history.pkl (Training History)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= CONTOH TESTING DENGAN PENJELASAN DETAIL =======\n",
    "\n",
    "def demo_single_prediction(model, label_encoder, audio_file):\n",
    "    \"\"\"\n",
    "    Demo prediksi single file dengan penjelasan detail\n",
    "    \"\"\"\n",
    "    print(f\"=== TESTING FILE: {os.path.basename(audio_file)} ===\")\n",
    "    \n",
    "    if not os.path.exists(audio_file):\n",
    "        print(f\"File not found: {audio_file}\")\n",
    "        return\n",
    "    \n",
    "    # Get actual verse from filename\n",
    "    actual_verse = int(os.path.basename(audio_file).split('.')[0][-3:])\n",
    "    actual_name = get_verse_name(actual_verse)\n",
    "    \n",
    "    print(f\"Expected: {actual_name}\")\n",
    "    \n",
    "    # Predict\n",
    "    result = predict_verse_with_details(model, label_encoder, audio_file)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Predicted: {result['verse_name']}\")\n",
    "        print(f\"Confidence: {result['confidence_percent']:.1f}%\")\n",
    "        \n",
    "        is_correct = result['verse_number'] == actual_verse\n",
    "        print(f\"Result: {'‚úì CORRECT' if is_correct else '‚úó WRONG'}\")\n",
    "        \n",
    "        if result['confidence'] < 0.5:\n",
    "            print(\"‚ö†Ô∏è  Low confidence - model might be uncertain\")\n",
    "        elif result['confidence'] > 0.9:\n",
    "            print(\"‚ú® High confidence - model is very sure\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to process audio file\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def demo_batch_testing(model, label_encoder, test_folder):\n",
    "    \"\"\"\n",
    "    Demo testing pada folder dengan analysis yang detail\n",
    "    \"\"\"\n",
    "    print(f\"=== BATCH TESTING: {os.path.basename(test_folder)} ===\")\n",
    "    \n",
    "    if not os.path.exists(test_folder):\n",
    "        print(f\"Folder not found: {test_folder}\")\n",
    "        return\n",
    "    \n",
    "    # Test semua files di folder\n",
    "    results = test_on_folder(model, label_encoder, test_folder)\n",
    "    \n",
    "    if results:\n",
    "        # Analysis per jenis ayat\n",
    "        print(f\"\\n=== PERFORMANCE ANALYSIS ===\")\n",
    "        \n",
    "        # Group by verse type\n",
    "        bismillah_results = [r for r in results if r['actual_verse'] == 0]\n",
    "        verse_results = [r for r in results if r['actual_verse'] > 0]\n",
    "        \n",
    "        if bismillah_results:\n",
    "            bismillah_acc = sum([r['correct'] for r in bismillah_results]) / len(bismillah_results)\n",
    "            print(f\"Bismillah detection: {bismillah_acc:.1%} ({len(bismillah_results)} samples)\")\n",
    "        \n",
    "        if verse_results:\n",
    "            verse_acc = sum([r['correct'] for r in verse_results]) / len(verse_results)\n",
    "            print(f\"Verse detection: {verse_acc:.1%} ({len(verse_results)} samples)\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        all_confidence = [r['confidence'] for r in results]\n",
    "        correct_confidence = [r['confidence'] for r in results if r['correct']]\n",
    "        wrong_confidence = [r['confidence'] for r in results if not r['correct']]\n",
    "        \n",
    "        print(f\"\\nConfidence Analysis:\")\n",
    "        print(f\"Average confidence: {np.mean(all_confidence):.3f}\")\n",
    "        if correct_confidence:\n",
    "            print(f\"Correct predictions avg confidence: {np.mean(correct_confidence):.3f}\")\n",
    "        if wrong_confidence:\n",
    "            print(f\"Wrong predictions avg confidence: {np.mean(wrong_confidence):.3f}\")\n",
    "        \n",
    "        # Show worst predictions\n",
    "        if wrong_confidence:\n",
    "            wrong_results = [r for r in results if not r['correct']]\n",
    "            worst_results = sorted(wrong_results, key=lambda x: x['confidence'], reverse=True)[:3]\n",
    "            \n",
    "            print(f\"\\nMost confident wrong predictions:\")\n",
    "            for r in worst_results:\n",
    "                print(f\"  {r['filename']}: {r['actual_verse_name']} -> {r['predicted_verse_name']} (conf: {r['confidence']:.3f})\")\n",
    "\n",
    "# Jalankan demo jika model sudah di-load\n",
    "print(\"=== DEMO FUNCTIONS READY ===\")\n",
    "print(\"Setelah model selesai training dan disimpan, Anda bisa menggunakan:\")\n",
    "print(\"1. demo_single_prediction(saved_model, saved_label_encoder, 'path/to/audio.mp3')\")\n",
    "print(\"2. demo_batch_testing(saved_model, saved_label_encoder, 'path/to/folder')\")\n",
    "print(\"\\nContoh usage:\")\n",
    "print(\"demo_single_prediction(saved_model, saved_label_encoder, r'd:\\\\new_project\\\\quran_detect\\\\sample_1\\\\078000.mp3')\")\n",
    "print(\"demo_batch_testing(saved_model, saved_label_encoder, r'd:\\\\new_project\\\\quran_detect\\\\sample_2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= TEST FILE SPESIFIK: test.mp3 =======\n",
    "\n",
    "def test_specific_file(model, label_encoder, file_path=\"test.mp3\"):\n",
    "    \"\"\"\n",
    "    Test file audio spesifik (test.mp3) dengan analisis detail\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéµ TESTING FILE: test.mp3\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Cek berbagai lokasi file test.mp3\n",
    "    possible_paths = [\n",
    "        file_path,  # Langsung dari parameter\n",
    "        os.path.join(os.getcwd(), file_path),  # Di working directory\n",
    "        os.path.join(r\"d:\\new_project\\quran_detect\", file_path),  # Di folder project\n",
    "    ]\n",
    "    \n",
    "    test_file_path = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            test_file_path = path\n",
    "            print(f\"‚úÖ File ditemukan: {path}\")\n",
    "            break\n",
    "    \n",
    "    if test_file_path is None:\n",
    "        print(\"‚ùå File test.mp3 tidak ditemukan!\")\n",
    "        print(\"üìç Lokasi yang dicari:\")\n",
    "        for path in possible_paths:\n",
    "            print(f\"   - {path}\")\n",
    "        print(\"\\nüí° Saran:\")\n",
    "        print(\"1. Pastikan file test.mp3 ada di salah satu lokasi di atas\")\n",
    "        print(\"2. Atau gunakan path lengkap: test_specific_file(model, label_encoder, r'path\\\\lengkap\\\\ke\\\\test.mp3')\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÇ Full path: {test_file_path}\")\n",
    "    \n",
    "    # Cek info file\n",
    "    try:\n",
    "        file_size = os.path.getsize(test_file_path)\n",
    "        print(f\"üìä File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Tidak bisa membaca info file\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "    print(\"üîç MEMPROSES AUDIO...\")\n",
    "    print(\"‚îÄ\" * 50)\n",
    "    \n",
    "    # Prediksi dengan detail\n",
    "    try:\n",
    "        result = predict_verse_with_details(model, label_encoder, test_file_path)\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"‚ùå Gagal memproses file audio!\")\n",
    "            print(\"üí° Kemungkinan penyebab:\")\n",
    "            print(\"   - Format file tidak didukung\")\n",
    "            print(\"   - File audio rusak\")\n",
    "            print(\"   - Audio terlalu pendek atau terlalu panjang\")\n",
    "            return None\n",
    "        \n",
    "        print(\"‚úÖ Audio berhasil diproses!\")\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üéØ HASIL PREDIKSI\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Tampilkan hasil prediksi\n",
    "        print(f\"üìù Prediksi: {result['verse_name']}\")\n",
    "        print(f\"üî¢ Label ID: {result['verse_number']}\")\n",
    "        print(f\"üìä Confidence: {result['confidence_percent']:.1f}%\")\n",
    "        \n",
    "        # Interpretasi confidence\n",
    "        confidence = result['confidence']\n",
    "        if confidence >= 0.9:\n",
    "            confidence_level = \"üü¢ SANGAT TINGGI\"\n",
    "            interpretation = \"Model sangat yakin dengan prediksinya\"\n",
    "        elif confidence >= 0.7:\n",
    "            confidence_level = \"üü° TINGGI\"\n",
    "            interpretation = \"Model cukup yakin dengan prediksinya\"\n",
    "        elif confidence >= 0.5:\n",
    "            confidence_level = \"üü† SEDANG\"\n",
    "            interpretation = \"Model agak ragu, perlu verifikasi manual\"\n",
    "        else:\n",
    "            confidence_level = \"üî¥ RENDAH\"\n",
    "            interpretation = \"Model tidak yakin, kemungkinan prediksi salah\"\n",
    "        \n",
    "        print(f\"üìà Tingkat Keyakinan: {confidence_level}\")\n",
    "        print(f\"üí≠ Interpretasi: {interpretation}\")\n",
    "        \n",
    "        # Detail tambahan\n",
    "        print(f\"\\nüìã Detail Teknis:\")\n",
    "        print(f\"   - File: {result['filename']}\")\n",
    "        print(f\"   - Raw confidence: {confidence:.6f}\")\n",
    "        \n",
    "        # Saran tindak lanjut\n",
    "        print(f\"\\nüí° Saran:\")\n",
    "        if confidence >= 0.8:\n",
    "            print(\"   ‚úÖ Prediksi dapat dipercaya\")\n",
    "        elif confidence >= 0.6:\n",
    "            print(\"   ‚ö†Ô∏è  Disarankan untuk cross-check manual\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Prediksi mungkin tidak akurat, coba file audio yang lebih jelas\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saat memproses: {str(e)}\")\n",
    "        print(\"üí° Coba restart kernel atau check file audio\")\n",
    "        return None\n",
    "\n",
    "# ======= JALANKAN TEST untuk test.mp3 =======\n",
    "\n",
    "print(\"üöÄ Menjalankan test untuk file test.mp3...\")\n",
    "print(\"‚è≥ Pastikan file test.mp3 sudah tersedia di folder project\")\n",
    "\n",
    "# Test menggunakan model yang sudah diload\n",
    "if 'saved_model' in locals() and 'saved_label_encoder' in locals():\n",
    "    result = test_specific_file(saved_model, saved_label_encoder, \"test.mp3\")\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nüéâ SUCCESS! File test.mp3 diprediksi sebagai: {result['verse_name']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Gagal memproses test.mp3. Pastikan file tersedia dan valid.\")\n",
    "else:\n",
    "    print(\"‚ùå Model belum di-load. Jalankan cell training terlebih dahulu!\")\n",
    "    print(\"üí° Atau load model dengan: model, label_encoder, _, _ = load_model_and_metadata()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c063dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= CUSTOM FILE TESTING =======\n",
    "# Gunakan cell ini untuk test file dengan nama atau path yang berbeda\n",
    "\n",
    "def quick_test_audio(file_path):\n",
    "    \"\"\"\n",
    "    Quick test untuk file audio apapun\n",
    "    \"\"\"\n",
    "    if 'saved_model' not in locals() or 'saved_label_encoder' not in locals():\n",
    "        print(\"‚ùå Model belum di-load!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üéµ Testing: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File tidak ditemukan: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    result = predict_verse_with_details(saved_model, saved_label_encoder, file_path)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"üìù Hasil: {result['verse_name']}\")\n",
    "        print(f\"üìä Confidence: {result['confidence_percent']:.1f}%\")\n",
    "        \n",
    "        if result['confidence'] >= 0.8:\n",
    "            print(\"‚úÖ Prediksi sangat yakin\")\n",
    "        elif result['confidence'] >= 0.6:\n",
    "            print(\"‚ö†Ô∏è  Prediksi cukup yakin\")\n",
    "        else:\n",
    "            print(\"‚ùå Prediksi kurang yakin\")\n",
    "    else:\n",
    "        print(\"‚ùå Gagal memproses file\")\n",
    "\n",
    "\n",
    "test_specific_file(saved_model, saved_label_encoder, 'test.mp3')\n",
    "\n",
    "# ======= CONTOH PENGGUNAAN =======\n",
    "print(\"üìù Contoh penggunaan:\")\n",
    "print(\"quick_test_audio('test.mp3')\")\n",
    "print(\"quick_test_audio(r'd:\\\\path\\\\to\\\\your\\\\audio.mp3')\")\n",
    "print(\"test_specific_file(saved_model, saved_label_encoder, 'your_file.mp3')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üí° INSTRUKSI PENGGUNAAN:\")\n",
    "print(\"1. Letakkan file test.mp3 di folder project ini\")\n",
    "print(\"2. Atau gunakan path lengkap ke file audio Anda\")\n",
    "print(\"3. Jalankan: quick_test_audio('nama_file.mp3')\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f128a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= IMPROVED AUDIO PREPROCESSING & MODEL =======\n",
    "\n",
    "import scipy.signal\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras import regularizers\n",
    "import scipy.stats\n",
    "\n",
    "def extract_advanced_features(file_path, max_length=256, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract features audio yang lebih comprehensive untuk deteksi yang lebih baik\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio dengan preprocessing\n",
    "        audio, sample_rate = librosa.load(file_path, sr=sr)\n",
    "        \n",
    "        # 1. Audio Preprocessing\n",
    "        # Remove silence dan normalize\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        \n",
    "        # Trim silence dari awal dan akhir\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "        \n",
    "        # Apply pre-emphasis filter\n",
    "        audio = scipy.signal.lfilter([1, -0.95], [1], audio)\n",
    "        \n",
    "        # 2. Feature Extraction yang lebih lengkap\n",
    "        features_list = []\n",
    "        \n",
    "        # MFCC dengan konfigurasi yang lebih baik\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20, n_fft=2048, hop_length=512)\n",
    "        mfcc_delta = librosa.feature.delta(mfccs)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        features_list.extend([mfccs, mfcc_delta, mfcc_delta2])\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, roll_percent=0.85)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, n_bands=6)\n",
    "        \n",
    "        features_list.extend([spectral_centroids, spectral_rolloff, spectral_bandwidth, spectral_contrast])\n",
    "        \n",
    "        # Rhythm features\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)\n",
    "        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        \n",
    "        # Chroma features (untuk pitch information)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        \n",
    "        # Tonnetz (harmonic features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)\n",
    "        \n",
    "        features_list.extend([zero_crossing_rate, chroma, tonnetz])\n",
    "        \n",
    "        # 3. Combine all features\n",
    "        combined_features = np.vstack(features_list)\n",
    "        \n",
    "        # 4. Feature normalization dan padding/truncation\n",
    "        # Normalize each feature type\n",
    "        scaler = RobustScaler()\n",
    "        combined_features = scaler.fit_transform(combined_features.T).T\n",
    "        \n",
    "        # Pad or truncate to fixed length\n",
    "        if combined_features.shape[1] < max_length:\n",
    "            pad_width = max_length - combined_features.shape[1]\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :max_length]\n",
    "        \n",
    "        # Transpose untuk format yang diharapkan model (time_steps, features)\n",
    "        return combined_features.T\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_improved_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Model yang diperbaiki dengan arsitektur yang lebih advanced\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # 1. Convolutional layers untuk pattern extraction\n",
    "    x = layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    \n",
    "    # Conv1D layers untuk temporal pattern\n",
    "    x = layers.Conv1D(64, kernel_size=5, activation='relu', padding='same')(layers.Reshape((input_shape[0], input_shape[1]))(inputs))\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # 2. Attention mechanism\n",
    "    attention = layers.Dense(128, activation='tanh')(x)\n",
    "    attention = layers.Dense(1, activation='softmax')(attention)\n",
    "    x = layers.Multiply()([x, attention])\n",
    "    \n",
    "    # 3. Bidirectional LSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "    \n",
    "    # 4. Dense layers dengan regularization\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Custom learning rate scheduler (Fixed)\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(WarmupCosineDecay, self).__init__()\n",
    "        self.initial_learning_rate = tf.cast(initial_learning_rate, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "        self.total_steps = tf.cast(total_steps, tf.float32)\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        \n",
    "        # Warmup phase\n",
    "        warmup_lr = (self.initial_learning_rate * step / self.warmup_steps)\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        cosine_lr = self.initial_learning_rate * 0.5 * (1 + tf.cos(\n",
    "            tf.constant(3.14159265359, dtype=tf.float32) * (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "        ))\n",
    "        \n",
    "        return tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)\n",
    "\n",
    "def load_data_improved():\n",
    "    \"\"\"\n",
    "    Load data dengan improved preprocessing\n",
    "    \"\"\"\n",
    "    base_path = r\"d:\\new_project\\quran_detect\"\n",
    "    \n",
    "    # Cek semua folder sample\n",
    "    available_folders = []\n",
    "    for i in range(1, 8):\n",
    "        folder_name = f'sample_{i}'\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            available_folders.append(folder_name)\n",
    "    \n",
    "    print(f\"Found {len(available_folders)} sample folders: {available_folders}\")\n",
    "    print(\"Using improved feature extraction...\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    file_info = []\n",
    "    \n",
    "    for folder in available_folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        print(f\"Processing {folder} with advanced features...\")\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(folder_path) if f.endswith('.mp3')])\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {folder}\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            # Use improved feature extraction\n",
    "            features = extract_advanced_features(file_path, max_length=256)\n",
    "            \n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                verse_num = int(file.split('.')[0][-3:])\n",
    "                y.append(verse_num)\n",
    "                \n",
    "                file_info.append({\n",
    "                    'folder': folder,\n",
    "                    'filename': file,\n",
    "                    'verse_label': verse_num\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  Failed to extract features from: {file}\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\n=== Improved Dataset Summary ===\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Feature shape per sample: {X[0].shape if len(X) > 0 else 'N/A'}\")\n",
    "    print(f\"Feature dimensions: {X.shape[2] if len(X) > 0 else 'N/A'} features per timestep\")\n",
    "    print(f\"Verse range: {min(y)} to {max(y)}\")\n",
    "    print(f\"Unique verses: {len(np.unique(y))}\")\n",
    "    \n",
    "    return X, y, file_info\n",
    "\n",
    "print(\"‚úÖ Improved preprocessing and model functions loaded!\")\n",
    "print(\"üìà Key improvements:\")\n",
    "print(\"   - Advanced audio preprocessing (noise reduction, normalization)\")\n",
    "print(\"   - Comprehensive feature extraction (MFCC + deltas, spectral, chroma, etc.)\")\n",
    "print(\"   - Improved model architecture (Conv1D + Attention + BiLSTM)\")\n",
    "print(\"   - Better optimizer with learning rate scheduling\")\n",
    "print(\"   - Regularization untuk prevent overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= IMPROVED TRAINING FUNCTION =======\n",
    "\n",
    "def train_improved_model(X, y, save_model=True, model_name=\"improved_quran_model\"):\n",
    "    \"\"\"\n",
    "    Training function dengan berbagai perbaikan untuk akurasi yang lebih tinggi\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting improved training...\")\n",
    "    \n",
    "    # 1. Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Convert to categorical\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "    \n",
    "    # 2. Data Augmentation untuk training set\n",
    "    def augment_audio_features(features, noise_factor=0.02):\n",
    "        \"\"\"Simple data augmentation\"\"\"\n",
    "        # Add small random noise\n",
    "        noise = np.random.normal(0, noise_factor, features.shape)\n",
    "        return features + noise\n",
    "    \n",
    "    # 3. Stratified split dengan data augmentation\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    # Use more sophisticated split\n",
    "    test_size = 0.15  # Smaller test set, larger training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=test_size, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # 4. Data augmentation for training set\n",
    "    print(\"Applying data augmentation...\")\n",
    "    X_train_aug = []\n",
    "    y_train_aug = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Original sample\n",
    "        X_train_aug.append(X_train[i])\n",
    "        y_train_aug.append(y_train[i])\n",
    "        \n",
    "        # Augmented samples (2 variations per original)\n",
    "        aug1 = augment_audio_features(X_train[i], noise_factor=0.01)\n",
    "        aug2 = augment_audio_features(X_train[i], noise_factor=0.02)\n",
    "        \n",
    "        X_train_aug.extend([aug1, aug2])\n",
    "        y_train_aug.extend([y_train[i], y_train[i]])\n",
    "    \n",
    "    X_train_aug = np.array(X_train_aug)\n",
    "    y_train_aug = np.array(y_train_aug)\n",
    "    \n",
    "    print(f\"After augmentation: {len(X_train_aug)} training samples\")\n",
    "    \n",
    "    # 5. Create improved model\n",
    "    model = create_improved_model((X.shape[1], X.shape[2]), num_classes)\n",
    "    \n",
    "    # 6. Advanced optimizer dengan learning rate scheduling\n",
    "    total_steps = (len(X_train_aug) // 16) * 100  # batch_size=16, epochs=100\n",
    "    warmup_steps = total_steps // 10\n",
    "    \n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        warmup_steps=warmup_steps,\n",
    "        total_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Use AdamW optimizer (better than standard Adam)\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    # 7. Compile model dengan custom metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'), \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.F1Score(name='f1_score')]\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 8. Advanced callbacks\n",
    "    model_dir = f\"model_saves_{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    callbacks = [\n",
    "        # Early stopping with patience\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpoint\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(model_dir, 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.7,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # CSV logger\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            os.path.join(model_dir, 'training_log.csv')\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 9. Determine training parameters\n",
    "    total_samples = len(X_train_aug)\n",
    "    if total_samples < 500:\n",
    "        batch_size = 8\n",
    "        epochs = 80\n",
    "    else:\n",
    "        batch_size = 16\n",
    "        epochs = 100\n",
    "    \n",
    "    print(f\"Training parameters:\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Learning rate: Adaptive with warmup\")\n",
    "    print(f\"  - Data augmentation: Applied\")\n",
    "    \n",
    "    # 10. Train model\n",
    "    print(f\"üéØ Starting training...\")\n",
    "    history = model.fit(\n",
    "        X_train_aug, y_train_aug,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 11. Evaluate model\n",
    "    print(f\"\\nüìà Final Evaluation:\")\n",
    "    train_loss, train_acc, train_top3, train_prec, train_rec, train_f1 = model.evaluate(X_train_aug, y_train_aug, verbose=0)\n",
    "    test_loss, test_acc, test_top3, test_prec, test_rec, test_f1 = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"Training - Accuracy: {train_acc:.4f}, Precision: {train_prec:.4f}, Recall: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"Testing  - Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {test_top3:.4f}\")\n",
    "    \n",
    "    # 12. Save model and metadata\n",
    "    if save_model:\n",
    "        save_improved_model_metadata(model, label_encoder, history, model_dir, {\n",
    "            'train_accuracy': train_acc,\n",
    "            'test_accuracy': test_acc,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'top3_accuracy': test_top3,\n",
    "            'data_augmentation': True,\n",
    "            'feature_extraction': 'advanced',\n",
    "            'model_type': 'improved_cnn_bilstm_attention'\n",
    "        })\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "def save_improved_model_metadata(model, label_encoder, history, model_dir, extra_metrics):\n",
    "    \"\"\"\n",
    "    Save improved model with enhanced metadata\n",
    "    \"\"\"\n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, \"improved_quran_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"‚úÖ Label encoder saved to: {encoder_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(model_dir, \"training_history.pkl\")\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"‚úÖ Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Enhanced metadata\n",
    "    metadata = {\n",
    "        \"model_version\": \"improved_v2\",\n",
    "        \"num_classes\": len(label_encoder.classes_),\n",
    "        \"verse_labels\": label_encoder.classes_.tolist(),\n",
    "        \"input_shape\": model.input_shape[1:],\n",
    "        \"total_epochs\": len(history.history['loss']),\n",
    "        \"best_val_accuracy\": max(history.history['val_accuracy']),\n",
    "        \"best_val_loss\": min(history.history['val_loss']),\n",
    "        \"final_metrics\": extra_metrics,\n",
    "        \"improvements\": [\n",
    "            \"Advanced audio preprocessing\",\n",
    "            \"Comprehensive feature extraction\",\n",
    "            \"CNN + BiLSTM + Attention architecture\", \n",
    "            \"AdamW optimizer with learning rate scheduling\",\n",
    "            \"Data augmentation\",\n",
    "            \"Regularization techniques\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"model_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"‚úÖ Enhanced metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"üéØ Improved training function ready!\")\n",
    "print(\"üìã Key improvements in training:\")\n",
    "print(\"   ‚úÖ Data augmentation untuk increase dataset size\")\n",
    "print(\"   ‚úÖ AdamW optimizer dengan learning rate scheduling\")\n",
    "print(\"   ‚úÖ Advanced callbacks dan monitoring\")\n",
    "print(\"   ‚úÖ Comprehensive evaluation metrics\")\n",
    "print(\"   ‚úÖ Better train/test split strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= ALTERNATIVE SIMPLIFIED TRAINING (JIKA ADA ERROR) =======\n",
    "\n",
    "def train_improved_model_simple(X, y, save_model=True, model_name=\"improved_simple\"):\n",
    "    \"\"\"\n",
    "    Training function yang disederhanakan jika ada masalah dengan advanced scheduler\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting simplified improved training...\")\n",
    "    \n",
    "    # 1. Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Convert to categorical\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "    \n",
    "    # 2. Data Augmentation\n",
    "    def augment_audio_features(features, noise_factor=0.02):\n",
    "        noise = np.random.normal(0, noise_factor, features.shape)\n",
    "        return features + noise\n",
    "    \n",
    "    # 3. Train-test split\n",
    "    test_size = 0.15\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=test_size, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # 4. Data augmentation\n",
    "    print(\"Applying data augmentation...\")\n",
    "    X_train_aug = []\n",
    "    y_train_aug = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Original sample\n",
    "        X_train_aug.append(X_train[i])\n",
    "        y_train_aug.append(y_train[i])\n",
    "        \n",
    "        # Augmented samples\n",
    "        aug1 = augment_audio_features(X_train[i], noise_factor=0.01)\n",
    "        aug2 = augment_audio_features(X_train[i], noise_factor=0.02)\n",
    "        \n",
    "        X_train_aug.extend([aug1, aug2])\n",
    "        y_train_aug.extend([y_train[i], y_train[i]])\n",
    "    \n",
    "    X_train_aug = np.array(X_train_aug)\n",
    "    y_train_aug = np.array(y_train_aug)\n",
    "    \n",
    "    print(f\"After augmentation: {len(X_train_aug)} training samples\")\n",
    "    \n",
    "    # 5. Create improved model\n",
    "    model = create_improved_model((X.shape[1], X.shape[2]), num_classes)\n",
    "    \n",
    "    # 6. Simplified optimizer (no custom scheduler)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    # 7. Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'), \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 8. Simplified callbacks\n",
    "    model_dir = f\"model_saves_{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(model_dir, 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.7,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 9. Training parameters\n",
    "    total_samples = len(X_train_aug)\n",
    "    if total_samples < 500:\n",
    "        batch_size = 8\n",
    "        epochs = 80\n",
    "    else:\n",
    "        batch_size = 16\n",
    "        epochs = 100\n",
    "    \n",
    "    print(f\"Training parameters:\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Optimizer: Simplified Adam\")\n",
    "    \n",
    "    # 10. Train model\n",
    "    print(f\"üéØ Starting training...\")\n",
    "    history = model.fit(\n",
    "        X_train_aug, y_train_aug,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 11. Evaluate model\n",
    "    print(f\"\\nüìà Final Evaluation:\")\n",
    "    try:\n",
    "        train_metrics = model.evaluate(X_train_aug, y_train_aug, verbose=0)\n",
    "        test_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        print(f\"Training - Accuracy: {train_metrics[1]:.4f}, Precision: {train_metrics[3]:.4f}, Recall: {train_metrics[4]:.4f}\")\n",
    "        print(f\"Testing  - Accuracy: {test_metrics[1]:.4f}, Precision: {test_metrics[3]:.4f}, Recall: {test_metrics[4]:.4f}\")\n",
    "        print(f\"Top-3 Accuracy: {test_metrics[2]:.4f}\")\n",
    "        \n",
    "        extra_metrics = {\n",
    "            'train_accuracy': train_metrics[1],\n",
    "            'test_accuracy': test_metrics[1],\n",
    "            'test_precision': test_metrics[3],\n",
    "            'test_recall': test_metrics[4],\n",
    "            'top3_accuracy': test_metrics[2],\n",
    "            'data_augmentation': True,\n",
    "            'feature_extraction': 'advanced',\n",
    "            'model_type': 'simplified_improved'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        extra_metrics = {\n",
    "            'train_accuracy': 0,\n",
    "            'test_accuracy': 0,\n",
    "            'model_type': 'simplified_improved'\n",
    "        }\n",
    "    \n",
    "    # 12. Save model\n",
    "    if save_model:\n",
    "        save_improved_model_metadata(model, label_encoder, history, model_dir, extra_metrics)\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "print(\"üîß Simplified training function ready!\")\n",
    "print(\"üí° Use this if the advanced training function has issues with TensorFlow versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82548fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= RESTART TRAINING DENGAN ERROR HANDLING =======\n",
    "\n",
    "print(\"üîÑ RESTARTING IMPROVED TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check apakah data sudah di-load\n",
    "if 'X_improved' in locals() and 'y_improved' in locals():\n",
    "    print(f\"‚úÖ Data sudah tersedia: {len(X_improved)} samples\")\n",
    "    \n",
    "    # Try training dengan error handling\n",
    "    try:\n",
    "        print(\"üöÄ Attempting simplified training (more stable)...\")\n",
    "        restart_model, restart_history, restart_label_encoder = train_improved_model_simple(\n",
    "            X_improved, y_improved, save_model=True, model_name=\"improved_restart\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéâ TRAINING BERHASIL!\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Plot hasil training\n",
    "        print(\"üìä Generating training plots...\")\n",
    "        restart_plot = plot_training_history(restart_history)\n",
    "        \n",
    "        print(f\"\\nüìÅ Model disimpan di: model_saves_improved_restart/\")\n",
    "        print(f\"üéØ Model siap untuk testing!\")\n",
    "        \n",
    "        # Quick evaluation\n",
    "        try:\n",
    "            final_metrics = restart_model.evaluate(X_improved[:50], \n",
    "                tf.keras.utils.to_categorical(LabelEncoder().fit_transform(y_improved[:50]), \n",
    "                len(np.unique(y_improved))), verbose=0)\n",
    "            print(f\"üìä Quick test accuracy: {final_metrics[1]:.3f}\")\n",
    "        except:\n",
    "            print(\"üìä Quick test skipped\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training gagal: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting:\")\n",
    "        print(\"1. Restart kernel jika perlu\")\n",
    "        print(\"2. Check memory availability\")  \n",
    "        print(\"3. Reduce batch size atau epochs\")\n",
    "        print(\"4. Verify TensorFlow installation\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Data belum di-load!\")\n",
    "    print(\"üí° Jalankan cell load data terlebih dahulu:\")\n",
    "    print(\"   X_improved, y_improved, file_info_improved = load_data_improved()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27998294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= ULTRA SIMPLE TRAINING (GUARANTEED TO WORK) =======\n",
    "\n",
    "def train_basic_improved_model(X, y, save_model=True, model_name=\"basic_improved\"):\n",
    "    \"\"\"\n",
    "    Training function yang sangat basic tapi menggunakan improved features\n",
    "    Guaranteed to work pada semua versi TensorFlow\n",
    "    \"\"\"\n",
    "    print(\"üîß Starting ultra-simple but improved training...\")\n",
    "    \n",
    "    # 1. Basic label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "    \n",
    "    # 2. Simple train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Feature shape: {X_train.shape}\")\n",
    "    \n",
    "    # 3. Create basic improved model (simplified architecture)\n",
    "    inputs = tf.keras.Input(shape=(X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Simple but effective architecture\n",
    "    x = layers.LSTM(64, return_sequences=True, dropout=0.3)(inputs)\n",
    "    x = layers.LSTM(32, return_sequences=False, dropout=0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    # 4. Basic optimizer and compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Basic Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 5. Simple callbacks\n",
    "    model_dir = f\"model_saves_{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 6. Train model\n",
    "    print(\"üéØ Starting basic training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=16,\n",
    "        epochs=50,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    print(f\"\\nüìà Results:\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # 8. Save model\n",
    "    if save_model:\n",
    "        model_path = os.path.join(model_dir, \"basic_improved_model.h5\")\n",
    "        model.save(model_path)\n",
    "        \n",
    "        encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "        with open(encoder_path, 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "        \n",
    "        metadata = {\n",
    "            \"model_type\": \"basic_improved\",\n",
    "            \"train_accuracy\": float(train_acc),\n",
    "            \"test_accuracy\": float(test_acc),\n",
    "            \"num_classes\": num_classes,\n",
    "            \"input_shape\": list(X.shape[1:]),\n",
    "            \"epochs_trained\": len(history.history['loss'])\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to: {model_dir}/\")\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "# ======= EXECUTE ULTRA SIMPLE TRAINING =======\n",
    "\n",
    "print(\"üöÄ STARTING GUARANTEED TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'X_improved' in locals() and 'y_improved' in locals():\n",
    "    print(f\"‚úÖ Using improved features: {X_improved.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Use ultra-simple training that's guaranteed to work\n",
    "        basic_model, basic_history, basic_encoder = train_basic_improved_model(\n",
    "            X_improved, y_improved, save_model=True, model_name=\"basic_improved\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéâ TRAINING SUCCESSFUL!\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"üìä Generating plots...\")\n",
    "        basic_plot = plot_training_history(basic_history)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Success! Model info:\")\n",
    "        print(f\"   - Architecture: Simple LSTM with improved features\")\n",
    "        print(f\"   - Features: {X_improved.shape[2]} advanced audio features\")\n",
    "        print(f\"   - Classes: {len(np.unique(y_improved))} verses\")\n",
    "        print(f\"   - Saved to: model_saves_basic_improved/\")\n",
    "        \n",
    "        # Test dengan sample data\n",
    "        try:\n",
    "            test_sample = X_improved[0:1]\n",
    "            test_pred = basic_model.predict(test_sample, verbose=0)\n",
    "            predicted_class = np.argmax(test_pred)\n",
    "            confidence = np.max(test_pred)\n",
    "            verse_number = basic_encoder.inverse_transform([predicted_class])[0]\n",
    "            \n",
    "            print(f\"\\nüß™ Quick test:\")\n",
    "            print(f\"   Sample prediction: {get_verse_name(verse_number)}\")\n",
    "            print(f\"   Confidence: {confidence:.3f}\")\n",
    "        except:\n",
    "            print(f\"\\nüß™ Quick test skipped\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Even basic training failed: {e}\")\n",
    "        print(\"üí° Try:\")\n",
    "        print(\"1. Restart kernel\")\n",
    "        print(\"2. Check TensorFlow installation\")\n",
    "        print(\"3. Verify data integrity\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Improved data not loaded!\")\n",
    "    print(\"üí° Run: X_improved, y_improved, _ = load_data_improved()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= TESTING BASIC IMPROVED MODEL =======\n",
    "\n",
    "def load_basic_model(model_dir=\"model_saves_basic_improved\"):\n",
    "    \"\"\"Load basic improved model\"\"\"\n",
    "    try:\n",
    "        model_path = os.path.join(model_dir, \"basic_improved_model.h5\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        \n",
    "        metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Basic model loaded successfully!\")\n",
    "        print(f\"   - Test accuracy: {metadata['test_accuracy']:.3f}\")\n",
    "        print(f\"   - Classes: {metadata['num_classes']}\")\n",
    "        \n",
    "        return model, label_encoder, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_with_basic_model(model, label_encoder, audio_file_path):\n",
    "    \"\"\"Predict using basic model dengan improved features\"\"\"\n",
    "    # Extract improved features\n",
    "    features = extract_advanced_features(audio_file_path)\n",
    "    if features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Reshape dan predict\n",
    "    features = features.reshape(1, features.shape[0], features.shape[1])\n",
    "    prediction = model.predict(features, verbose=0)\n",
    "    \n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "    verse_number = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return verse_number, confidence\n",
    "\n",
    "def test_basic_model_performance(model, label_encoder, test_folder, max_files=20):\n",
    "    \"\"\"Test basic model performance\"\"\"\n",
    "    if not os.path.exists(test_folder):\n",
    "        print(f\"‚ùå Test folder not found: {test_folder}\")\n",
    "        return\n",
    "    \n",
    "    files = sorted([f for f in os.listdir(test_folder) if f.endswith('.mp3')])[:max_files]\n",
    "    \n",
    "    print(f\"üß™ Testing {len(files)} files...\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    for file in tqdm(files, desc=\"Testing\"):\n",
    "        file_path = os.path.join(test_folder, file)\n",
    "        actual_verse = int(file.split('.')[0][-3:])\n",
    "        \n",
    "        try:\n",
    "            predicted_verse, confidence = predict_with_basic_model(model, label_encoder, file_path)\n",
    "            if predicted_verse is not None:\n",
    "                is_correct = (predicted_verse == actual_verse)\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'file': file,\n",
    "                    'actual': actual_verse,\n",
    "                    'predicted': predicted_verse,\n",
    "                    'confidence': confidence,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        avg_confidence = np.mean([r['confidence'] for r in results])\n",
    "        \n",
    "        print(f\"\\nüìä BASIC MODEL PERFORMANCE:\")\n",
    "        print(f\"   Accuracy: {accuracy:.1%} ({correct}/{total})\")\n",
    "        print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(f\"\\nüìã Sample results:\")\n",
    "        for result in results[:5]:\n",
    "            status = \"‚úÖ\" if result['correct'] else \"‚ùå\"\n",
    "            actual_name = get_verse_name(result['actual'])\n",
    "            pred_name = get_verse_name(result['predicted'])\n",
    "            print(f\"   {status} {result['file']}: {actual_name} -> {pred_name} ({result['confidence']:.2f})\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"‚ùå No valid results\")\n",
    "        return []\n",
    "\n",
    "# ======= TEST BASIC MODEL =======\n",
    "\n",
    "print(\"üß™ TESTING BASIC IMPROVED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load basic model jika sudah di-train\n",
    "if 'basic_model' in locals() and 'basic_encoder' in locals():\n",
    "    print(\"‚úÖ Using model from current session\")\n",
    "    test_model = basic_model\n",
    "    test_encoder = basic_encoder\n",
    "else:\n",
    "    print(\"üîÑ Loading saved model...\")\n",
    "    test_model, test_encoder, test_metadata = load_basic_model()\n",
    "\n",
    "if test_model is not None:\n",
    "    # Test dengan file test.mp3 jika ada\n",
    "    test_files = [\"test.mp3\", \"sample_1/078001.mp3\", \"sample_2/078005.mp3\"]\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        full_path = test_file if os.path.exists(test_file) else os.path.join(r\"d:\\new_project\\quran_detect\", test_file)\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"\\nüéµ Testing: {os.path.basename(full_path)}\")\n",
    "            try:\n",
    "                pred_verse, conf = predict_with_basic_model(test_model, test_encoder, full_path)\n",
    "                if pred_verse is not None:\n",
    "                    verse_name = get_verse_name(pred_verse)\n",
    "                    print(f\"   Prediction: {verse_name}\")\n",
    "                    print(f\"   Confidence: {conf:.3f}\")\n",
    "                    \n",
    "                    if conf > 0.7:\n",
    "                        print(\"   ‚úÖ High confidence\")\n",
    "                    elif conf > 0.5:\n",
    "                        print(\"   ‚ö†Ô∏è  Medium confidence\")\n",
    "                    else:\n",
    "                        print(\"   ‚ùå Low confidence\")\n",
    "                else:\n",
    "                    print(\"   ‚ùå Failed to process\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Test pada folder sample\n",
    "    sample_folder = r\"d:\\new_project\\quran_detect\\sample_1\"\n",
    "    if os.path.exists(sample_folder):\n",
    "        print(f\"\\nüî¨ Comprehensive testing on sample folder...\")\n",
    "        test_results = test_basic_model_performance(test_model, test_encoder, sample_folder, max_files=10)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model available for testing\")\n",
    "    print(\"üí° Train model first using the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23877e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= JALANKAN TRAINING DENGAN MODEL YANG DIPERBAIKI =======\n",
    "\n",
    "print(\"üöÄ MEMULAI TRAINING DENGAN MODEL YANG DIPERBAIKI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data dengan preprocessing yang diperbaiki\n",
    "print(\"üì• Loading data dengan improved preprocessing...\")\n",
    "X_improved, y_improved, file_info_improved = load_data_improved()\n",
    "\n",
    "if len(X_improved) > 0:\n",
    "    print(f\"‚úÖ Data berhasil diload: {len(X_improved)} samples\")\n",
    "    print(f\"üìä Feature shape: {X_improved.shape}\")\n",
    "    \n",
    "    # Bandingkan dengan data sebelumnya\n",
    "    if 'X' in locals():\n",
    "        print(f\"üìà Improvement comparison:\")\n",
    "        print(f\"   - Previous features per timestep: {X.shape[2] if len(X.shape) > 2 else 'N/A'}\")\n",
    "        print(f\"   - Improved features per timestep: {X_improved.shape[2]}\")\n",
    "        print(f\"   - Previous sequence length: {X.shape[1] if len(X.shape) > 1 else 'N/A'}\")\n",
    "        print(f\"   - Improved sequence length: {X_improved.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Starting improved training...\")\n",
    "    print(\"‚è≥ This may take longer due to:\")\n",
    "    print(\"   - Advanced feature extraction\")\n",
    "    print(\"   - Data augmentation\") \n",
    "    print(\"   - More complex model architecture\")\n",
    "    \n",
    "    # Train dengan model yang diperbaiki - try advanced first, fallback to simple\n",
    "    try:\n",
    "        print(\"üöÄ Attempting advanced training with custom scheduler...\")\n",
    "        improved_model, improved_history, improved_label_encoder = train_improved_model(\n",
    "            X_improved, y_improved, save_model=True, model_name=\"improved_v2\"\n",
    "        )\n",
    "        print(\"‚úÖ Advanced training successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Advanced training failed: {e}\")\n",
    "        print(\"üîÑ Falling back to simplified training...\")\n",
    "        improved_model, improved_history, improved_label_encoder = train_improved_model_simple(\n",
    "            X_improved, y_improved, save_model=True, model_name=\"improved_simple\"\n",
    "        )\n",
    "        print(\"‚úÖ Simplified training successful!\")\n",
    "    \n",
    "    print(f\"\\nüéâ TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Plot training history\n",
    "    print(\"üìä Generating improved training plots...\")\n",
    "    improved_plot = plot_training_history(improved_history)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model yang diperbaiki telah selesai!\")\n",
    "    print(f\"üìÅ Files disimpan di: model_saves_improved_v2/\")\n",
    "    print(f\"üéØ Sekarang coba test dengan model yang diperbaiki!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Gagal load data. Pastikan folder sample tersedia dan valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= TESTING MODEL YANG DIPERBAIKI =======\n",
    "\n",
    "def load_improved_model(model_dir=\"model_saves_improved_v2\"):\n",
    "    \"\"\"\n",
    "    Load improved model yang telah disimpan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = os.path.join(model_dir, \"improved_quran_model.h5\")\n",
    "        \n",
    "        # Custom objects untuk load model\n",
    "        custom_objects = {\n",
    "            'WarmupCosineDecay': WarmupCosineDecay,\n",
    "            'AdamW': AdamW\n",
    "        }\n",
    "        \n",
    "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "        print(f\"‚úÖ Improved model loaded from: {model_path}\")\n",
    "        \n",
    "        # Load label encoder\n",
    "        encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        print(f\"‚úÖ Label encoder loaded\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(model_dir, \"model_metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"‚úÖ Metadata loaded\")\n",
    "        \n",
    "        return model, label_encoder, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading improved model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_with_improved_model(model, label_encoder, audio_file_path):\n",
    "    \"\"\"\n",
    "    Prediksi menggunakan improved model dengan preprocessing yang sama\n",
    "    \"\"\"\n",
    "    # Extract features dengan improved preprocessing\n",
    "    features = extract_advanced_features(audio_file_path)\n",
    "    if features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Reshape untuk prediksi\n",
    "    features = features.reshape(1, features.shape[0], features.shape[1])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features, verbose=0)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top3_indices = np.argsort(prediction[0])[-3:][::-1]\n",
    "    top3_probs = prediction[0][top3_indices]\n",
    "    \n",
    "    # Convert back to original labels\n",
    "    verse_number = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    top3_verses = label_encoder.inverse_transform(top3_indices)\n",
    "    \n",
    "    return {\n",
    "        'verse_number': verse_number,\n",
    "        'confidence': confidence,\n",
    "        'top3_predictions': list(zip(top3_verses, top3_probs))\n",
    "    }\n",
    "\n",
    "def compare_models_performance(original_model, improved_model, original_encoder, improved_encoder, test_folder):\n",
    "    \"\"\"\n",
    "    Bandingkan performa model original vs improved\n",
    "    \"\"\"\n",
    "    print(\"üîç PERBANDINGAN PERFORMA MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(test_folder):\n",
    "        print(f\"‚ùå Test folder tidak ditemukan: {test_folder}\")\n",
    "        return\n",
    "    \n",
    "    files = sorted([f for f in os.listdir(test_folder) if f.endswith('.mp3')])[:10]  # Test 10 files pertama\n",
    "    \n",
    "    results_comparison = []\n",
    "    \n",
    "    for file in tqdm(files, desc=\"Comparing models\"):\n",
    "        file_path = os.path.join(test_folder, file)\n",
    "        actual_verse = int(file.split('.')[0][-3:])\n",
    "        \n",
    "        # Original model prediction\n",
    "        try:\n",
    "            orig_verse, orig_conf = predict_verse(original_model, original_encoder, file_path)\n",
    "            orig_correct = (orig_verse == actual_verse) if orig_verse is not None else False\n",
    "        except:\n",
    "            orig_verse, orig_conf, orig_correct = None, 0, False\n",
    "        \n",
    "        # Improved model prediction\n",
    "        try:\n",
    "            improved_result = predict_with_improved_model(improved_model, improved_encoder, file_path)\n",
    "            if improved_result:\n",
    "                imp_verse = improved_result['verse_number']\n",
    "                imp_conf = improved_result['confidence']\n",
    "                imp_correct = (imp_verse == actual_verse)\n",
    "                imp_top3 = improved_result['top3_predictions']\n",
    "            else:\n",
    "                imp_verse, imp_conf, imp_correct, imp_top3 = None, 0, False, []\n",
    "        except:\n",
    "            imp_verse, imp_conf, imp_correct, imp_top3 = None, 0, False, []\n",
    "        \n",
    "        results_comparison.append({\n",
    "            'file': file,\n",
    "            'actual': actual_verse,\n",
    "            'original_pred': orig_verse,\n",
    "            'original_conf': orig_conf,\n",
    "            'original_correct': orig_correct,\n",
    "            'improved_pred': imp_verse,\n",
    "            'improved_conf': imp_conf,\n",
    "            'improved_correct': imp_correct,\n",
    "            'improved_top3': imp_top3\n",
    "        })\n",
    "    \n",
    "    # Calculate statistics\n",
    "    orig_accuracy = sum([r['original_correct'] for r in results_comparison]) / len(results_comparison)\n",
    "    imp_accuracy = sum([r['improved_correct'] for r in results_comparison]) / len(results_comparison)\n",
    "    \n",
    "    orig_avg_conf = np.mean([r['original_conf'] for r in results_comparison if r['original_conf'] > 0])\n",
    "    imp_avg_conf = np.mean([r['improved_conf'] for r in results_comparison if r['improved_conf'] > 0])\n",
    "    \n",
    "    print(f\"\\nüìä HASIL PERBANDINGAN:\")\n",
    "    print(f\"{'Model':<15} {'Accuracy':<12} {'Avg Confidence':<15}\")\n",
    "    print(\"-\" * 42)\n",
    "    print(f\"{'Original':<15} {orig_accuracy:.2%}      {orig_avg_conf:.3f}\")\n",
    "    print(f\"{'Improved':<15} {imp_accuracy:.2%}      {imp_avg_conf:.3f}\")\n",
    "    \n",
    "    improvement = imp_accuracy - orig_accuracy\n",
    "    print(f\"\\nüéØ Improvement: {improvement:.2%}\")\n",
    "    \n",
    "    # Show detailed results\n",
    "    print(f\"\\nüìã Detailed Results:\")\n",
    "    for result in results_comparison:\n",
    "        actual_name = get_verse_name(result['actual'])\n",
    "        orig_status = \"‚úÖ\" if result['original_correct'] else \"‚ùå\"\n",
    "        imp_status = \"‚úÖ\" if result['improved_correct'] else \"‚ùå\"\n",
    "        \n",
    "        print(f\"{result['file']}: {actual_name}\")\n",
    "        print(f\"  Original: {orig_status} {get_verse_name(result['original_pred']) if result['original_pred'] is not None else 'Failed'} ({result['original_conf']:.2f})\")\n",
    "        print(f\"  Improved: {imp_status} {get_verse_name(result['improved_pred']) if result['improved_pred'] is not None else 'Failed'} ({result['improved_conf']:.2f})\")\n",
    "        \n",
    "        if result['improved_top3']:\n",
    "            top3_str = \", \".join([f\"{get_verse_name(v)}({p:.2f})\" for v, p in result['improved_top3']])\n",
    "            print(f\"  Top-3: {top3_str}\")\n",
    "        print()\n",
    "    \n",
    "    return results_comparison\n",
    "\n",
    "# Test improved model jika sudah di-train\n",
    "print(\"üß™ TESTING IMPROVED MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Coba load improved model\n",
    "improved_model_loaded, improved_encoder_loaded, improved_metadata = load_improved_model()\n",
    "\n",
    "if improved_model_loaded is not None:\n",
    "    print(f\"‚úÖ Improved model berhasil di-load!\")\n",
    "    print(f\"üìä Model info:\")\n",
    "    print(f\"   - Accuracy: {improved_metadata['final_metrics']['test_accuracy']:.4f}\")\n",
    "    print(f\"   - F1 Score: {improved_metadata['final_metrics']['test_f1']:.4f}\")\n",
    "    print(f\"   - Top-3 Accuracy: {improved_metadata['final_metrics']['top3_accuracy']:.4f}\")\n",
    "    \n",
    "    # Test dengan file test.mp3 jika ada\n",
    "    test_file_path = \"test.mp3\"\n",
    "    if os.path.exists(test_file_path):\n",
    "        print(f\"\\nüéµ Testing dengan file: {test_file_path}\")\n",
    "        improved_result = predict_with_improved_model(improved_model_loaded, improved_encoder_loaded, test_file_path)\n",
    "        \n",
    "        if improved_result:\n",
    "            print(f\"üìù Prediksi: {get_verse_name(improved_result['verse_number'])}\")\n",
    "            print(f\"üìä Confidence: {improved_result['confidence']:.3f}\")\n",
    "            print(f\"ü•á Top-3 predictions:\")\n",
    "            for i, (verse, prob) in enumerate(improved_result['top3_predictions'], 1):\n",
    "                print(f\"   {i}. {get_verse_name(verse)}: {prob:.3f}\")\n",
    "    \n",
    "    # Perbandingan dengan model original jika tersedia\n",
    "    if 'saved_model' in locals() and 'saved_label_encoder' in locals():\n",
    "        print(f\"\\nüîÑ Comparing with original model...\")\n",
    "        comparison = compare_models_performance(\n",
    "            saved_model, improved_model_loaded,\n",
    "            saved_label_encoder, improved_encoder_loaded,\n",
    "            r\"d:\\new_project\\quran_detect\\sample_1\"\n",
    "        )\n",
    "else:\n",
    "    print(\"‚ùå Improved model belum di-train. Jalankan cell training terlebih dahulu!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
