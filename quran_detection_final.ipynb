{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c11e6f",
   "metadata": {},
   "source": [
    "# Quran Verse Detection - Surah An-Naba\n",
    "\n",
    "## Dataset Structure\n",
    "Dataset ini berisi rekaman audio Surah An-Naba dari 7 pembaca Quran yang berbeda:\n",
    "\n",
    "### Folder Structure:\n",
    "```\n",
    "quran_detect/\n",
    "â”œâ”€â”€ sample_1/     # Pembaca 1\n",
    "â”œâ”€â”€ sample_2/     # Pembaca 2\n",
    "â”œâ”€â”€ sample_3/     # Pembaca 3\n",
    "â”œâ”€â”€ sample_4/     # Pembaca 4\n",
    "â”œâ”€â”€ sample_5/     # Pembaca 5\n",
    "â”œâ”€â”€ sample_6/     # Pembaca 6\n",
    "â””â”€â”€ sample_7/     # Pembaca 7\n",
    "```\n",
    "\n",
    "### File Naming Convention:\n",
    "Setiap folder berisi 41 file audio dengan format:\n",
    "- `078000.mp3` â†’ **Bismillah** (Pembuka) â†’ **Label 0**\n",
    "- `078001.mp3` â†’ **Ayat 1** â†’ **Label 1**\n",
    "- `078002.mp3` â†’ **Ayat 2** â†’ **Label 2**\n",
    "- ...\n",
    "- `078040.mp3` â†’ **Ayat 40** â†’ **Label 40**\n",
    "\n",
    "### Total Dataset:\n",
    "- **7 pembaca** Ã— **41 ayat** = **287 audio samples**\n",
    "- **41 kelas** (0-40): 1 Bismillah + 40 Ayat\n",
    "\n",
    "## Tujuan Model:\n",
    "Model dilatih untuk mendeteksi ayat mana yang sedang dibacakan ketika diberi audio testing baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca8e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "TensorFlow version: 2.18.1\n",
      "Librosa version: 0.10.2.post1\n"
     ]
    }
   ],
   "source": [
    "# ======= IMPORT LIBRARIES =======\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import scipy.signal\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de91e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced audio preprocessing function ready!\n",
      "ğŸ“Š Features extracted: MFCC+deltas, Spectral, Chroma, Tonnetz (~80 features)\n"
     ]
    }
   ],
   "source": [
    "# ======= ADVANCED AUDIO PREPROCESSING =======\n",
    "\n",
    "def extract_advanced_features(file_path, max_length=256, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features for better verse detection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio dengan preprocessing\n",
    "        audio, sample_rate = librosa.load(file_path, sr=sr)\n",
    "        \n",
    "        # 1. Audio Preprocessing\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)  # Remove silence\n",
    "        audio = scipy.signal.lfilter([1, -0.95], [1], audio)  # Pre-emphasis\n",
    "        \n",
    "        # 2. Feature Extraction\n",
    "        features_list = []\n",
    "        \n",
    "        # MFCC features with deltas\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20, n_fft=2048, hop_length=512)\n",
    "        mfcc_delta = librosa.feature.delta(mfccs)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        features_list.extend([mfccs, mfcc_delta, mfcc_delta2])\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, roll_percent=0.85)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, n_bands=6)\n",
    "        features_list.extend([spectral_centroids, spectral_rolloff, spectral_bandwidth, spectral_contrast])\n",
    "        \n",
    "        # Rhythm and harmonic features\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)\n",
    "        features_list.extend([zero_crossing_rate, chroma, tonnetz])\n",
    "        \n",
    "        # 3. Combine and normalize features\n",
    "        combined_features = np.vstack(features_list)\n",
    "        \n",
    "        # Robust scaling\n",
    "        scaler = RobustScaler()\n",
    "        combined_features = scaler.fit_transform(combined_features.T).T\n",
    "        \n",
    "        # 4. Pad or truncate to fixed length\n",
    "        if combined_features.shape[1] < max_length:\n",
    "            pad_width = max_length - combined_features.shape[1]\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :max_length]\n",
    "        \n",
    "        return combined_features.T  # Shape: (time_steps, features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Advanced audio preprocessing function ready!\")\n",
    "print(\"ğŸ“Š Features extracted: MFCC+deltas, Spectral, Chroma, Tonnetz (~80 features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5367d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loading function ready!\n"
     ]
    }
   ],
   "source": [
    "# ======= DATA LOADING FUNCTION =======\n",
    "\n",
    "def load_quran_data():\n",
    "    \"\"\"\n",
    "    Load Quran verse data dari multiple pembaca dengan improved preprocessing\n",
    "    \"\"\"\n",
    "    base_path = r\"d:\\new_project\\quran_detect\"\n",
    "    \n",
    "    # Detect available sample folders\n",
    "    available_folders = []\n",
    "    for i in range(1, 8):  # sample_1 to sample_7\n",
    "        folder_name = f'sample_{i}'\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            available_folders.append(folder_name)\n",
    "        else:\n",
    "            print(f\"Warning: {folder_name} not found\")\n",
    "    \n",
    "    print(f\"Found {len(available_folders)} sample folders: {available_folders}\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    file_info = []\n",
    "    \n",
    "    for folder in available_folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        print(f\"Processing {folder}...\")\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(folder_path) if f.endswith('.mp3')])\n",
    "        print(f\"  Found {len(files)} audio files\")\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Extracting features from {folder}\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_advanced_features(file_path)\n",
    "            \n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                \n",
    "                # Extract verse number: 078000.mp3 -> 0, 078001.mp3 -> 1, etc.\n",
    "                verse_num = int(file.split('.')[0][-3:])\n",
    "                y.append(verse_num)\n",
    "                \n",
    "                file_info.append({\n",
    "                    'folder': folder,\n",
    "                    'filename': file,\n",
    "                    'verse_label': verse_num\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  Failed to process: {file}\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Dataset summary\n",
    "    print(f\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Total pembaca: {len(available_folders)}\")\n",
    "    print(f\"Feature shape: {X[0].shape if len(X) > 0 else 'N/A'}\")\n",
    "    print(f\"Verse range: {min(y)} to {max(y)}\")\n",
    "    print(f\"Unique verses: {len(np.unique(y))}\")\n",
    "    \n",
    "    # Check distribution\n",
    "    verse_counts = np.bincount(y)\n",
    "    print(f\"\\nSamples per verse (should be {len(available_folders)} each):\")\n",
    "    for verse_id in range(min(10, len(verse_counts))):\n",
    "        if verse_counts[verse_id] > 0:\n",
    "            verse_name = \"Bismillah\" if verse_id == 0 else f\"Ayat {verse_id}\"\n",
    "            print(f\"  {verse_name} (ID:{verse_id}): {verse_counts[verse_id]} samples\")\n",
    "    \n",
    "    return X, y, file_info\n",
    "\n",
    "print(\"âœ… Data loading function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29aff867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model architecture ready!\n",
      "ğŸ—ï¸  Architecture: Conv1D â†’ BiLSTM â†’ Dense layers with regularization\n"
     ]
    }
   ],
   "source": [
    "# ======= MODEL ARCHITECTURE =======\n",
    "\n",
    "def create_quran_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create optimized model for Quran verse detection\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers for pattern extraction\n",
    "    x = layers.Conv1D(64, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Bidirectional LSTM for temporal modeling\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.3))(x)\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "print(\"âœ… Model architecture ready!\")\n",
    "print(\"ğŸ—ï¸  Architecture: Conv1D â†’ BiLSTM â†’ Dense layers with regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbeca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training function ready!\n"
     ]
    }
   ],
   "source": [
    "# ======= TRAINING FUNCTION =======\n",
    "\n",
    "def train_quran_model(X, y, model_name=\"quran_model_final\", save_model=True):\n",
    "    \"\"\"\n",
    "    Train the Quran verse detection model\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Starting Quran verse detection training...\")\n",
    "    \n",
    "    # 1. Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "    \n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # 2. Data augmentation\n",
    "    def augment_features(features, noise_factor=0.02):\n",
    "        noise = np.random.normal(0, noise_factor, features.shape)\n",
    "        return features + noise\n",
    "    \n",
    "    # 3. Train-test split\n",
    "    test_size = 0.15\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=test_size, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # 4. Apply data augmentation\n",
    "    print(\"Applying data augmentation...\")\n",
    "    X_train_aug = []\n",
    "    y_train_aug = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Original sample\n",
    "        X_train_aug.append(X_train[i])\n",
    "        y_train_aug.append(y_train[i])\n",
    "        \n",
    "        # Augmented samples\n",
    "        aug1 = augment_features(X_train[i], 0.01)\n",
    "        aug2 = augment_features(X_train[i], 0.02)\n",
    "        \n",
    "        X_train_aug.extend([aug1, aug2])\n",
    "        y_train_aug.extend([y_train[i], y_train[i]])\n",
    "    \n",
    "    X_train_aug = np.array(X_train_aug)\n",
    "    y_train_aug = np.array(y_train_aug)\n",
    "    \n",
    "    print(f\"After augmentation: {len(X_train_aug)} training samples\")\n",
    "    \n",
    "    # 5. Create and compile model\n",
    "    model = create_quran_model((X.shape[1], X.shape[2]), num_classes)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“Š Model summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # 6. Callbacks\n",
    "    model_dir = f\"model_saves_{model_name}\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(model_dir, 'best_model.h5'),\n",
    "            monitor='val_accuracy', save_best_only=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.7, patience=5, min_lr=1e-7, verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 7. Training parameters\n",
    "    batch_size = 16 if len(X_train_aug) > 500 else 8\n",
    "    epochs = 80\n",
    "    \n",
    "    print(f\"Training parameters:\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    \n",
    "    # 8. Train model\n",
    "    print(\"ğŸ¯ Starting training...\")\n",
    "    history = model.fit(\n",
    "        X_train_aug, y_train_aug,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 9. Evaluation\n",
    "    print(\"\\nğŸ“ˆ Final Evaluation:\")\n",
    "    train_metrics = model.evaluate(X_train_aug, y_train_aug, verbose=0)\n",
    "    test_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"Training - Accuracy: {train_metrics[1]:.4f}, Top-3: {train_metrics[2]:.4f}\")\n",
    "    print(f\"Testing  - Accuracy: {test_metrics[1]:.4f}, Top-3: {test_metrics[2]:.4f}\")\n",
    "    \n",
    "    # 10. Save model and metadata\n",
    "    if save_model:\n",
    "        save_model_and_metadata(model, label_encoder, history, model_dir, {\n",
    "            'train_accuracy': train_metrics[1],\n",
    "            'test_accuracy': test_metrics[1],\n",
    "            'train_top3': train_metrics[2],\n",
    "            'test_top3': test_metrics[2]\n",
    "        })\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "def save_model_and_metadata(model, label_encoder, history, model_dir, metrics):\n",
    "    \"\"\"Save model, encoder, and metadata\"\"\"\n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, \"quran_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ… Model saved: {model_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"âœ… Label encoder saved: {encoder_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(model_dir, \"training_history.pkl\")\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"model_version\": \"final_v1\",\n",
    "        \"num_classes\": len(label_encoder.classes_),\n",
    "        \"verse_labels\": label_encoder.classes_.tolist(),\n",
    "        \"input_shape\": model.input_shape[1:],\n",
    "        \"total_epochs\": len(history.history['loss']),\n",
    "        \"best_val_accuracy\": max(history.history['val_accuracy']),\n",
    "        \"metrics\": metrics,\n",
    "        \"features\": \"Advanced audio features (MFCC+deltas, spectral, chroma, tonnetz)\",\n",
    "        \"architecture\": \"Conv1D + BiLSTM + Dense with regularization\"\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"âœ… Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82775158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Visualization function ready!\n"
     ]
    }
   ],
   "source": [
    "# ======= VISUALIZATION FUNCTION =======\n",
    "\n",
    "def plot_training_results(history):\n",
    "    \"\"\"\n",
    "    Plot training history with Plotly\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Training & Validation Loss', 'Training & Validation Accuracy',\n",
    "                       'Top-3 Accuracy', 'Learning Rate'),\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['loss'], \n",
    "                  mode='lines', name='Training Loss', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_loss'], \n",
    "                  mode='lines', name='Validation Loss', line=dict(color='orange')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['accuracy'], \n",
    "                  mode='lines', name='Training Accuracy', line=dict(color='blue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_accuracy'], \n",
    "                  mode='lines', name='Validation Accuracy', line=dict(color='lightblue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Top-3 accuracy\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['top_3_accuracy'], \n",
    "                  mode='lines', name='Train Top-3', line=dict(color='green')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(epochs), y=history.history['val_top_3_accuracy'], \n",
    "                  mode='lines', name='Val Top-3', line=dict(color='lightgreen')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Learning rate if available\n",
    "    if 'lr' in history.history:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(epochs), y=history.history['lr'], \n",
    "                      mode='lines', name='Learning Rate', line=dict(color='purple')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Quran Verse Detection - Training Progress',\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Epochs\")\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Top-3 Accuracy\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Learning Rate\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "print(\"âœ… Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b150d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prediction and testing functions ready!\n"
     ]
    }
   ],
   "source": [
    "# ======= PREDICTION AND TESTING FUNCTIONS =======\n",
    "\n",
    "def get_verse_name(verse_number):\n",
    "    \"\"\"Convert verse number to readable name\"\"\"\n",
    "    if verse_number == 0:\n",
    "        return \"Bismillah (Pembuka)\"\n",
    "    elif 1 <= verse_number <= 40:\n",
    "        return f\"Ayat {verse_number}\"\n",
    "    else:\n",
    "        return f\"Unknown ({verse_number})\"\n",
    "\n",
    "def load_trained_model(model_dir=\"model_saves_quran_model_final\"):\n",
    "    \"\"\"Load trained model and components\"\"\"\n",
    "    try:\n",
    "        model_path = os.path.join(model_dir, \"quran_model.h5\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        encoder_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        \n",
    "        metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"âœ… Model loaded successfully!\")\n",
    "        print(f\"   Test accuracy: {metadata['metrics']['test_accuracy']:.3f}\")\n",
    "        print(f\"   Top-3 accuracy: {metadata['metrics']['test_top3']:.3f}\")\n",
    "        \n",
    "        return model, label_encoder, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_verse(model, label_encoder, audio_file_path):\n",
    "    \"\"\"Predict verse from audio file\"\"\"\n",
    "    # Extract features\n",
    "    features = extract_advanced_features(audio_file_path)\n",
    "    if features is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    features = features.reshape(1, features.shape[0], features.shape[1])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features, verbose=0)\n",
    "    \n",
    "    # Get top predictions\n",
    "    top3_indices = np.argsort(prediction[0])[-3:][::-1]\n",
    "    top3_probs = prediction[0][top3_indices]\n",
    "    \n",
    "    # Convert to verse numbers\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "    verse_number = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    top3_verses = label_encoder.inverse_transform(top3_indices)\n",
    "    \n",
    "    return verse_number, confidence, list(zip(top3_verses, top3_probs))\n",
    "\n",
    "def test_audio_file(model, label_encoder, audio_file_path):\n",
    "    \"\"\"Test single audio file with detailed output\"\"\"\n",
    "    print(f\"ğŸµ Testing: {os.path.basename(audio_file_path)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(audio_file_path):\n",
    "        print(f\"âŒ File not found: {audio_file_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        verse_number, confidence, top3 = predict_verse(model, label_encoder, audio_file_path)\n",
    "        \n",
    "        if verse_number is not None:\n",
    "            print(f\"ğŸ“ Prediction: {get_verse_name(verse_number)}\")\n",
    "            print(f\"ğŸ“Š Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "            \n",
    "            # Confidence interpretation\n",
    "            if confidence >= 0.8:\n",
    "                print(\"âœ… Very High Confidence\")\n",
    "            elif confidence >= 0.6:\n",
    "                print(\"ğŸŸ¡ Good Confidence\")\n",
    "            elif confidence >= 0.4:\n",
    "                print(\"âš ï¸  Medium Confidence\")\n",
    "            else:\n",
    "                print(\"âŒ Low Confidence\")\n",
    "            \n",
    "            print(f\"\\nğŸ¥‡ Top 3 predictions:\")\n",
    "            for i, (verse, prob) in enumerate(top3, 1):\n",
    "                print(f\"   {i}. {get_verse_name(verse)}: {prob:.3f}\")\n",
    "        else:\n",
    "            print(\"âŒ Failed to process audio\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "def test_folder_performance(model, label_encoder, test_folder, max_files=20):\n",
    "    \"\"\"Test model performance on a folder\"\"\"\n",
    "    if not os.path.exists(test_folder):\n",
    "        print(f\"âŒ Folder not found: {test_folder}\")\n",
    "        return\n",
    "    \n",
    "    files = sorted([f for f in os.listdir(test_folder) if f.endswith('.mp3')])[:max_files]\n",
    "    \n",
    "    print(f\"ğŸ§ª Testing {len(files)} files from {os.path.basename(test_folder)}\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    for file in tqdm(files, desc=\"Testing\"):\n",
    "        file_path = os.path.join(test_folder, file)\n",
    "        actual_verse = int(file.split('.')[0][-3:])\n",
    "        \n",
    "        try:\n",
    "            predicted_verse, confidence, _ = predict_verse(model, label_encoder, file_path)\n",
    "            if predicted_verse is not None:\n",
    "                is_correct = (predicted_verse == actual_verse)\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'file': file,\n",
    "                    'actual': actual_verse,\n",
    "                    'predicted': predicted_verse,\n",
    "                    'confidence': confidence,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {file}: {e}\")\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        avg_confidence = np.mean([r['confidence'] for r in results])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"   Accuracy: {accuracy:.1%} ({correct}/{total})\")\n",
    "        print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
    "        \n",
    "        # Show examples\n",
    "        print(f\"\\nğŸ“‹ Sample results:\")\n",
    "        for result in results[:5]:\n",
    "            status = \"âœ…\" if result['correct'] else \"âŒ\"\n",
    "            actual_name = get_verse_name(result['actual'])\n",
    "            pred_name = get_verse_name(result['predicted'])\n",
    "            print(f\"   {status} {actual_name} -> {pred_name} ({result['confidence']:.2f})\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"âŒ No valid results\")\n",
    "        return []\n",
    "\n",
    "print(\"âœ… Prediction and testing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6399baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING QURAN VERSE DETECTION SYSTEM\n",
      "============================================================\n",
      "ğŸ“¥ Loading Quran dataset...\n",
      "Found 7 sample folders: ['sample_1', 'sample_2', 'sample_3', 'sample_4', 'sample_5', 'sample_6', 'sample_7']\n",
      "Processing sample_1...\n",
      "  Found 41 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_1:   0%|          | 0/41 [00:00<?, ?it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=792\n",
      "  warnings.warn(\n",
      "Extracting features from sample_1:   2%|â–         | 1/41 [00:01<01:17,  1.93s/it]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=792\n",
      "  warnings.warn(\n",
      "Extracting features from sample_1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:08<00:00,  4.74it/s]\n",
      "Extracting features from sample_1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:08<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_2...\n",
      "  Found 41 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:06<00:00,  6.22it/s]\n",
      "Extracting features from sample_2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:06<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_3...\n",
      "  Found 41 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_3:   0%|          | 0/41 [00:00<?, ?it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=984\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3:   2%|â–         | 1/41 [00:00<00:04,  9.66it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=984\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3:  17%|â–ˆâ–‹        | 7/41 [00:00<00:03,  8.73it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=944\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3:  22%|â–ˆâ–ˆâ–       | 9/41 [00:00<00:03,  9.62it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=944\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:02<00:02,  7.48it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=974\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=974\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:04<00:00,  8.19it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=902\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=902\n",
      "  warnings.warn(\n",
      "Extracting features from sample_3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:05<00:00,  6.94it/s]\n",
      "Extracting features from sample_3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:05<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_4...\n",
      "  Found 41 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_4:   5%|â–         | 2/41 [00:00<00:05,  7.42it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "Extracting features from sample_4:  10%|â–‰         | 4/41 [00:00<00:04,  9.20it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=909\n",
      "  warnings.warn(\n",
      "Extracting features from sample_4:  10%|â–‰         | 4/41 [00:00<00:04,  9.20it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=884\n",
      "  warnings.warn(\n",
      "Extracting features from sample_4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:06<00:00,  6.67it/s]\n",
      "Extracting features from sample_4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:06<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_5...\n",
      "  Found 40 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.04it/s]\n",
      "Extracting features from sample_5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_6...\n",
      "  Found 40 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_6:  15%|â–ˆâ–Œ        | 6/40 [00:00<00:03,  8.88it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=936\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=936\n",
      "  warnings.warn(\n",
      "Extracting features from sample_6:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/40 [00:02<00:02,  7.75it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=904\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=904\n",
      "  warnings.warn(\n",
      "Extracting features from sample_6:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:03<00:00,  8.59it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=888\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=888\n",
      "  warnings.warn(\n",
      "Extracting features from sample_6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.52it/s]\n",
      "Extracting features from sample_6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:05<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample_7...\n",
      "  Found 41 audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features from sample_7:   5%|â–         | 2/41 [00:00<00:04,  9.08it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=999\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=999\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  10%|â–‰         | 4/41 [00:00<00:03,  9.95it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=631\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=631\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=990\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  15%|â–ˆâ–        | 6/41 [00:00<00:03, 10.84it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=990\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  15%|â–ˆâ–        | 6/41 [00:00<00:03, 10.84it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1008\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1008\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=865\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  20%|â–ˆâ–‰        | 8/41 [00:00<00:02, 11.05it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=865\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  24%|â–ˆâ–ˆâ–       | 10/41 [00:00<00:02, 11.01it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=846\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  29%|â–ˆâ–ˆâ–‰       | 12/41 [00:01<00:02, 10.98it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=846\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:02<00:02,  8.50it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=873\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=873\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:03<00:01,  8.35it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=982\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:03<00:00,  9.01it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=982\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:03<00:00,  9.01it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=918\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=918\n",
      "  warnings.warn(\n",
      "Extracting features from sample_7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:05<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Summary ===\n",
      "Total samples: 285\n",
      "Total pembaca: 7\n",
      "Feature shape: (256, 89)\n",
      "Verse range: 0 to 40\n",
      "Unique verses: 41\n",
      "\n",
      "Samples per verse (should be 7 each):\n",
      "  Bismillah (ID:0): 5 samples\n",
      "  Ayat 1 (ID:1): 7 samples\n",
      "  Ayat 2 (ID:2): 7 samples\n",
      "  Ayat 3 (ID:3): 7 samples\n",
      "  Ayat 4 (ID:4): 7 samples\n",
      "  Ayat 5 (ID:5): 7 samples\n",
      "  Ayat 6 (ID:6): 7 samples\n",
      "  Ayat 7 (ID:7): 7 samples\n",
      "  Ayat 8 (ID:8): 7 samples\n",
      "  Ayat 9 (ID:9): 7 samples\n",
      "\n",
      "âœ… Dataset loaded successfully!\n",
      "ğŸ“Š Total samples: 285\n",
      "ğŸµ Audio features per sample: 89\n",
      "â±ï¸  Sequence length: 256\n",
      "ğŸ“ Verses to detect: 41 (0=Bismillah, 1-40=Ayat 1-40)\n",
      "\n",
      "ğŸ“‹ Sample data:\n",
      "   sample_1/078000.mp3 -> Bismillah (Pembuka)\n",
      "   sample_1/078001.mp3 -> Ayat 1\n",
      "   sample_1/078002.mp3 -> Ayat 2\n",
      "\n",
      "ğŸ¯ Ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======= MAIN EXECUTION: LOAD DATA =======\n",
    "\n",
    "print(\"ğŸš€ STARTING QURAN VERSE DETECTION SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"ğŸ“¥ Loading Quran dataset...\")\n",
    "X, y, file_info = load_quran_data()\n",
    "\n",
    "if len(X) > 0:\n",
    "    print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Total samples: {len(X)}\")\n",
    "    print(f\"ğŸµ Audio features per sample: {X.shape[2]}\")\n",
    "    print(f\"â±ï¸  Sequence length: {X.shape[1]}\")\n",
    "    print(f\"ğŸ“ Verses to detect: {len(np.unique(y))} (0=Bismillah, 1-40=Ayat 1-40)\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nğŸ“‹ Sample data:\")\n",
    "    for i in range(min(3, len(file_info))):\n",
    "        info = file_info[i]\n",
    "        verse_name = get_verse_name(info['verse_label'])\n",
    "        print(f\"   {info['folder']}/{info['filename']} -> {verse_name}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Ready for training!\")\n",
    "else:\n",
    "    print(\"âŒ Failed to load data. Please check:\")\n",
    "    print(\"   1. Folder path is correct\")\n",
    "    print(\"   2. Audio files exist in sample_1, sample_2, etc.\")\n",
    "    print(\"   3. Files are in MP3 format with correct naming (078000.mp3, etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5442cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ TRAINING QURAN VERSE DETECTION MODEL\n",
      "============================================================\n",
      "ğŸš€ Starting Quran verse detection training...\n",
      "Number of classes: 41\n",
      "Classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40]\n",
      "Training samples: 242\n",
      "Test samples: 43\n",
      "Applying data augmentation...\n",
      "After augmentation: 726 training samples\n",
      "ğŸ“Š Model summary:\n",
      "After augmentation: 726 training samples\n",
      "ğŸ“Š Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,544</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,665</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m89\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m28,544\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚       \u001b[38;5;34m263,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m164,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚        \u001b[38;5;34m33,024\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)             â”‚         \u001b[38;5;34m2,665\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">559,401</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m559,401\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">558,505</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m558,505\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters:\n",
      "  Batch size: 16\n",
      "  Epochs: 80\n",
      "ğŸ¯ Starting training...\n",
      "Epoch 1/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.0333 - loss: 4.3478 - top_3_accuracy: 0.0805\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04651, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04651, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 133ms/step - accuracy: 0.0333 - loss: 4.3453 - top_3_accuracy: 0.0805 - val_accuracy: 0.0465 - val_loss: 3.6967 - val_top_3_accuracy: 0.1163 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "Epoch 2/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.0564 - loss: 3.8895 - top_3_accuracy: 0.0962\n",
      "Epoch 2: val_accuracy did not improve from 0.04651\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.0562 - loss: 3.8881 - top_3_accuracy: 0.0963 - val_accuracy: 0.0233 - val_loss: 3.6761 - val_top_3_accuracy: 0.2093 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.04651\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.0562 - loss: 3.8881 - top_3_accuracy: 0.0963 - val_accuracy: 0.0233 - val_loss: 3.6761 - val_top_3_accuracy: 0.2093 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.0528 - loss: 3.6535 - top_3_accuracy: 0.1671\n",
      "Epoch 3: val_accuracy improved from 0.04651 to 0.09302, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.04651 to 0.09302, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.0528 - loss: 3.6530 - top_3_accuracy: 0.1669 - val_accuracy: 0.0930 - val_loss: 3.6340 - val_top_3_accuracy: 0.1628 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "Epoch 4/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.0784 - loss: 3.6091 - top_3_accuracy: 0.1889\n",
      "Epoch 4: val_accuracy improved from 0.09302 to 0.11628, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.09302 to 0.11628, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.0781 - loss: 3.6082 - top_3_accuracy: 0.1887 - val_accuracy: 0.1163 - val_loss: 3.5500 - val_top_3_accuracy: 0.2326 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "Epoch 5/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.1143 - loss: 3.3704 - top_3_accuracy: 0.2480\n",
      "Epoch 5: val_accuracy improved from 0.11628 to 0.13953, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.11628 to 0.13953, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.1140 - loss: 3.3711 - top_3_accuracy: 0.2477 - val_accuracy: 0.1395 - val_loss: 3.4302 - val_top_3_accuracy: 0.2791 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "Epoch 6/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.1069 - loss: 3.3309 - top_3_accuracy: 0.2503\n",
      "Epoch 6: val_accuracy did not improve from 0.13953\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.1072 - loss: 3.3291 - top_3_accuracy: 0.2508 - val_accuracy: 0.1163 - val_loss: 3.2830 - val_top_3_accuracy: 0.2791 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.13953\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.1072 - loss: 3.3291 - top_3_accuracy: 0.2508 - val_accuracy: 0.1163 - val_loss: 3.2830 - val_top_3_accuracy: 0.2791 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.1370 - loss: 3.1174 - top_3_accuracy: 0.3161\n",
      "Epoch 7: val_accuracy improved from 0.13953 to 0.20930, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.13953 to 0.20930, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 0.1374 - loss: 3.1162 - top_3_accuracy: 0.3166 - val_accuracy: 0.2093 - val_loss: 3.0869 - val_top_3_accuracy: 0.4419 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "Epoch 8/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.1783 - loss: 2.8537 - top_3_accuracy: 0.3881 \n",
      "Epoch 8: val_accuracy did not improve from 0.20930\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.1781 - loss: 2.8551 - top_3_accuracy: 0.3873 - val_accuracy: 0.1395 - val_loss: 2.9389 - val_top_3_accuracy: 0.3953 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.20930\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.1781 - loss: 2.8551 - top_3_accuracy: 0.3873 - val_accuracy: 0.1395 - val_loss: 2.9389 - val_top_3_accuracy: 0.3953 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.2000 - loss: 2.7894 - top_3_accuracy: 0.4326\n",
      "Epoch 9: val_accuracy improved from 0.20930 to 0.27907, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.20930 to 0.27907, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.2000 - loss: 2.7892 - top_3_accuracy: 0.4323 - val_accuracy: 0.2791 - val_loss: 2.7683 - val_top_3_accuracy: 0.4419 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "Epoch 10/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.2390 - loss: 2.5610 - top_3_accuracy: 0.4779\n",
      "Epoch 10: val_accuracy did not improve from 0.27907\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.2392 - loss: 2.5609 - top_3_accuracy: 0.4783 - val_accuracy: 0.2791 - val_loss: 2.7296 - val_top_3_accuracy: 0.4186 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.27907\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.2392 - loss: 2.5609 - top_3_accuracy: 0.4783 - val_accuracy: 0.2791 - val_loss: 2.7296 - val_top_3_accuracy: 0.4186 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.2725 - loss: 2.3907 - top_3_accuracy: 0.5078\n",
      "Epoch 11: val_accuracy did not improve from 0.27907\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.2718 - loss: 2.3918 - top_3_accuracy: 0.5076 - val_accuracy: 0.2326 - val_loss: 2.7517 - val_top_3_accuracy: 0.5116 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.27907\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.2718 - loss: 2.3918 - top_3_accuracy: 0.5076 - val_accuracy: 0.2326 - val_loss: 2.7517 - val_top_3_accuracy: 0.5116 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.2833 - loss: 2.3569 - top_3_accuracy: 0.5637\n",
      "Epoch 12: val_accuracy improved from 0.27907 to 0.32558, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.27907 to 0.32558, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.2836 - loss: 2.3553 - top_3_accuracy: 0.5641 - val_accuracy: 0.3256 - val_loss: 2.6350 - val_top_3_accuracy: 0.4651 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "Epoch 13/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.3130 - loss: 2.2432 - top_3_accuracy: 0.5866\n",
      "Epoch 13: val_accuracy did not improve from 0.32558\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.3130 - loss: 2.2425 - top_3_accuracy: 0.5867 - val_accuracy: 0.2791 - val_loss: 2.9526 - val_top_3_accuracy: 0.4419 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.32558\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.3130 - loss: 2.2425 - top_3_accuracy: 0.5867 - val_accuracy: 0.2791 - val_loss: 2.9526 - val_top_3_accuracy: 0.4419 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3817 - loss: 2.0731 - top_3_accuracy: 0.6398\n",
      "Epoch 14: val_accuracy did not improve from 0.32558\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.3813 - loss: 2.0733 - top_3_accuracy: 0.6400 - val_accuracy: 0.3256 - val_loss: 2.5927 - val_top_3_accuracy: 0.5581 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.32558\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.3813 - loss: 2.0733 - top_3_accuracy: 0.6400 - val_accuracy: 0.3256 - val_loss: 2.5927 - val_top_3_accuracy: 0.5581 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.3820 - loss: 1.9396 - top_3_accuracy: 0.6919\n",
      "Epoch 15: val_accuracy improved from 0.32558 to 0.37209, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.32558 to 0.37209, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.3812 - loss: 1.9400 - top_3_accuracy: 0.6913 - val_accuracy: 0.3721 - val_loss: 2.7440 - val_top_3_accuracy: 0.5581 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "Epoch 16/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.3814 - loss: 1.8644 - top_3_accuracy: 0.7119\n",
      "Epoch 16: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.3819 - loss: 1.8643 - top_3_accuracy: 0.7117 - val_accuracy: 0.3721 - val_loss: 2.6749 - val_top_3_accuracy: 0.4884 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.3819 - loss: 1.8643 - top_3_accuracy: 0.7117 - val_accuracy: 0.3721 - val_loss: 2.6749 - val_top_3_accuracy: 0.4884 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.4280 - loss: 1.6608 - top_3_accuracy: 0.7407\n",
      "Epoch 17: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.4280 - loss: 1.6621 - top_3_accuracy: 0.7403 - val_accuracy: 0.3256 - val_loss: 2.6322 - val_top_3_accuracy: 0.6047 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.4280 - loss: 1.6621 - top_3_accuracy: 0.7403 - val_accuracy: 0.3256 - val_loss: 2.6322 - val_top_3_accuracy: 0.6047 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.4655 - loss: 1.6873 - top_3_accuracy: 0.7244\n",
      "Epoch 18: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.4653 - loss: 1.6865 - top_3_accuracy: 0.7247 - val_accuracy: 0.3721 - val_loss: 2.5996 - val_top_3_accuracy: 0.5814 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.37209\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.4653 - loss: 1.6865 - top_3_accuracy: 0.7247 - val_accuracy: 0.3721 - val_loss: 2.5996 - val_top_3_accuracy: 0.5814 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4990 - loss: 1.4771 - top_3_accuracy: 0.7956 \n",
      "Epoch 19: val_accuracy did not improve from 0.37209\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.4987 - loss: 1.4780 - top_3_accuracy: 0.7954 - val_accuracy: 0.2558 - val_loss: 2.7210 - val_top_3_accuracy: 0.6047 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.37209\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.4987 - loss: 1.4780 - top_3_accuracy: 0.7954 - val_accuracy: 0.2558 - val_loss: 2.7210 - val_top_3_accuracy: 0.6047 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5117 - loss: 1.4125 - top_3_accuracy: 0.8060\n",
      "Epoch 20: val_accuracy improved from 0.37209 to 0.41860, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.37209 to 0.41860, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.5116 - loss: 1.4130 - top_3_accuracy: 0.8060 - val_accuracy: 0.4186 - val_loss: 2.4110 - val_top_3_accuracy: 0.7209 - learning_rate: 7.0000e-04\n",
      "Epoch 21/80\n",
      "Epoch 21/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5522 - loss: 1.2651 - top_3_accuracy: 0.8442 \n",
      "Epoch 21: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.5524 - loss: 1.2647 - top_3_accuracy: 0.8443 - val_accuracy: 0.3256 - val_loss: 2.4998 - val_top_3_accuracy: 0.6744 - learning_rate: 7.0000e-04\n",
      "Epoch 22/80\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.5524 - loss: 1.2647 - top_3_accuracy: 0.8443 - val_accuracy: 0.3256 - val_loss: 2.4998 - val_top_3_accuracy: 0.6744 - learning_rate: 7.0000e-04\n",
      "Epoch 22/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.5925 - loss: 1.2328 - top_3_accuracy: 0.8486\n",
      "Epoch 22: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.5924 - loss: 1.2327 - top_3_accuracy: 0.8486 - val_accuracy: 0.3488 - val_loss: 2.5778 - val_top_3_accuracy: 0.6047 - learning_rate: 7.0000e-04\n",
      "Epoch 23/80\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.5924 - loss: 1.2327 - top_3_accuracy: 0.8486 - val_accuracy: 0.3488 - val_loss: 2.5778 - val_top_3_accuracy: 0.6047 - learning_rate: 7.0000e-04\n",
      "Epoch 23/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6385 - loss: 1.0972 - top_3_accuracy: 0.8831\n",
      "Epoch 23: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.6382 - loss: 1.0979 - top_3_accuracy: 0.8832 - val_accuracy: 0.3488 - val_loss: 2.6193 - val_top_3_accuracy: 0.6512 - learning_rate: 7.0000e-04\n",
      "Epoch 24/80\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.41860\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.6382 - loss: 1.0979 - top_3_accuracy: 0.8832 - val_accuracy: 0.3488 - val_loss: 2.6193 - val_top_3_accuracy: 0.6512 - learning_rate: 7.0000e-04\n",
      "Epoch 24/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6435 - loss: 1.0471 - top_3_accuracy: 0.8845\n",
      "Epoch 24: val_accuracy improved from 0.41860 to 0.48837, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 24: val_accuracy improved from 0.41860 to 0.48837, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.6432 - loss: 1.0475 - top_3_accuracy: 0.8845 - val_accuracy: 0.4884 - val_loss: 2.4865 - val_top_3_accuracy: 0.6512 - learning_rate: 7.0000e-04\n",
      "Epoch 25/80\n",
      "Epoch 25/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6126 - loss: 1.0202 - top_3_accuracy: 0.9026\n",
      "Epoch 25: val_accuracy did not improve from 0.48837\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.6130 - loss: 1.0200 - top_3_accuracy: 0.9027 - val_accuracy: 0.3256 - val_loss: 2.4766 - val_top_3_accuracy: 0.6977 - learning_rate: 7.0000e-04\n",
      "Epoch 26/80\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.48837\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.6130 - loss: 1.0200 - top_3_accuracy: 0.9027 - val_accuracy: 0.3256 - val_loss: 2.4766 - val_top_3_accuracy: 0.6977 - learning_rate: 7.0000e-04\n",
      "Epoch 26/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7300 - loss: 0.8551 - top_3_accuracy: 0.9302\n",
      "Epoch 26: val_accuracy did not improve from 0.48837\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.7295 - loss: 0.8561 - top_3_accuracy: 0.9301 - val_accuracy: 0.3953 - val_loss: 2.4731 - val_top_3_accuracy: 0.6977 - learning_rate: 4.9000e-04\n",
      "Epoch 27/80\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.48837\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.7295 - loss: 0.8561 - top_3_accuracy: 0.9301 - val_accuracy: 0.3953 - val_loss: 2.4731 - val_top_3_accuracy: 0.6977 - learning_rate: 4.9000e-04\n",
      "Epoch 27/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6702 - loss: 0.9262 - top_3_accuracy: 0.9173\n",
      "Epoch 27: val_accuracy improved from 0.48837 to 0.51163, saving model to model_saves_quran_model_final\\best_model.h5\n",
      "\n",
      "Epoch 27: val_accuracy improved from 0.48837 to 0.51163, saving model to model_saves_quran_model_final\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6704 - loss: 0.9254 - top_3_accuracy: 0.9176 - val_accuracy: 0.5116 - val_loss: 2.4329 - val_top_3_accuracy: 0.7209 - learning_rate: 4.9000e-04\n",
      "Epoch 28/80\n",
      "Epoch 28/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7377 - loss: 0.7629 - top_3_accuracy: 0.9624\n",
      "Epoch 28: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.7374 - loss: 0.7635 - top_3_accuracy: 0.9621 - val_accuracy: 0.4884 - val_loss: 2.6571 - val_top_3_accuracy: 0.7209 - learning_rate: 4.9000e-04\n",
      "Epoch 29/80\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.7374 - loss: 0.7635 - top_3_accuracy: 0.9621 - val_accuracy: 0.4884 - val_loss: 2.6571 - val_top_3_accuracy: 0.7209 - learning_rate: 4.9000e-04\n",
      "Epoch 29/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7557 - loss: 0.7197 - top_3_accuracy: 0.9413\n",
      "Epoch 29: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.7552 - loss: 0.7208 - top_3_accuracy: 0.9411 - val_accuracy: 0.4651 - val_loss: 2.6381 - val_top_3_accuracy: 0.7442 - learning_rate: 4.9000e-04\n",
      "Epoch 30/80\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.7552 - loss: 0.7208 - top_3_accuracy: 0.9411 - val_accuracy: 0.4651 - val_loss: 2.6381 - val_top_3_accuracy: 0.7442 - learning_rate: 4.9000e-04\n",
      "Epoch 30/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6733 - loss: 0.8207 - top_3_accuracy: 0.9534\n",
      "Epoch 30: val_accuracy did not improve from 0.51163\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.6745 - loss: 0.8184 - top_3_accuracy: 0.9538 - val_accuracy: 0.4651 - val_loss: 2.5539 - val_top_3_accuracy: 0.6977 - learning_rate: 4.9000e-04\n",
      "Epoch 31/80\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.51163\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.6745 - loss: 0.8184 - top_3_accuracy: 0.9538 - val_accuracy: 0.4651 - val_loss: 2.5539 - val_top_3_accuracy: 0.6977 - learning_rate: 4.9000e-04\n",
      "Epoch 31/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7578 - loss: 0.7436 - top_3_accuracy: 0.9535\n",
      "Epoch 31: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.7578 - loss: 0.7429 - top_3_accuracy: 0.9536 - val_accuracy: 0.4186 - val_loss: 2.7143 - val_top_3_accuracy: 0.6744 - learning_rate: 3.4300e-04\n",
      "Epoch 32/80\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.7578 - loss: 0.7429 - top_3_accuracy: 0.9536 - val_accuracy: 0.4186 - val_loss: 2.7143 - val_top_3_accuracy: 0.6744 - learning_rate: 3.4300e-04\n",
      "Epoch 32/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7323 - loss: 0.7584 - top_3_accuracy: 0.9491\n",
      "Epoch 32: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.7324 - loss: 0.7576 - top_3_accuracy: 0.9493 - val_accuracy: 0.3721 - val_loss: 2.7166 - val_top_3_accuracy: 0.6512 - learning_rate: 3.4300e-04\n",
      "Epoch 33/80\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.7324 - loss: 0.7576 - top_3_accuracy: 0.9493 - val_accuracy: 0.3721 - val_loss: 2.7166 - val_top_3_accuracy: 0.6512 - learning_rate: 3.4300e-04\n",
      "Epoch 33/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7583 - loss: 0.6194 - top_3_accuracy: 0.9637\n",
      "Epoch 33: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.7582 - loss: 0.6203 - top_3_accuracy: 0.9637 - val_accuracy: 0.4884 - val_loss: 2.7738 - val_top_3_accuracy: 0.7209 - learning_rate: 3.4300e-04\n",
      "Epoch 34/80\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.7582 - loss: 0.6203 - top_3_accuracy: 0.9637 - val_accuracy: 0.4884 - val_loss: 2.7738 - val_top_3_accuracy: 0.7209 - learning_rate: 3.4300e-04\n",
      "Epoch 34/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7524 - loss: 0.6860 - top_3_accuracy: 0.9688\n",
      "Epoch 34: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.7529 - loss: 0.6852 - top_3_accuracy: 0.9688 - val_accuracy: 0.4186 - val_loss: 2.8229 - val_top_3_accuracy: 0.6512 - learning_rate: 3.4300e-04\n",
      "Epoch 35/80\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.51163\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.7529 - loss: 0.6852 - top_3_accuracy: 0.9688 - val_accuracy: 0.4186 - val_loss: 2.8229 - val_top_3_accuracy: 0.6512 - learning_rate: 3.4300e-04\n",
      "Epoch 35/80\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8339 - loss: 0.5370 - top_3_accuracy: 0.9677\n",
      "Epoch 35: val_accuracy did not improve from 0.51163\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.8334 - loss: 0.5380 - top_3_accuracy: 0.9677 - val_accuracy: 0.4651 - val_loss: 2.6827 - val_top_3_accuracy: 0.6977 - learning_rate: 3.4300e-04\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.51163\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "\u001b[1m46/46\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.8334 - loss: 0.5380 - top_3_accuracy: 0.9677 - val_accuracy: 0.4651 - val_loss: 2.6827 - val_top_3_accuracy: 0.6977 - learning_rate: 3.4300e-04\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "ğŸ“ˆ Final Evaluation:\n",
      "\n",
      "ğŸ“ˆ Final Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Accuracy: 0.8512, Top-3: 0.9793\n",
      "Testing  - Accuracy: 0.4186, Top-3: 0.7209\n",
      "âœ… Model saved: model_saves_quran_model_final\\quran_model.h5\n",
      "âœ… Label encoder saved: model_saves_quran_model_final\\label_encoder.pkl\n",
      "âœ… Metadata saved: model_saves_quran_model_final\\metadata.json\n",
      "\n",
      "ğŸ‰ TRAINING COMPLETED!\n",
      "==================================================\n",
      "ğŸ“Š Generating training plots...\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x",
         "y": [
          4.2316412925720215,
          3.8229711055755615,
          3.630014181137085,
          3.570200204849243,
          3.4025821685791016,
          3.245603561401367,
          3.064079523086548,
          2.917437791824341,
          2.7833805084228516,
          2.5589499473571777,
          2.444488286972046,
          2.281399965286255,
          2.213473081588745,
          2.0813071727752686,
          1.9575347900390625,
          1.8596974611282349,
          1.71917724609375,
          1.6518464088439941,
          1.519640564918518,
          1.4329884052276611,
          1.2470672130584717,
          1.2286463975906372,
          1.1289887428283691,
          1.067521333694458,
          1.010118842124939,
          0.8988180160522461,
          0.8868331909179688,
          0.7904847860336304,
          0.7680129408836365,
          0.7121173739433289,
          0.7150260806083679,
          0.7215125560760498,
          0.6590036153793335,
          0.6492983102798462,
          0.5833911895751953
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "orange"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x",
         "y": [
          3.696729898452759,
          3.6760895252227783,
          3.6340396404266357,
          3.5500495433807373,
          3.4301722049713135,
          3.2830395698547363,
          3.0868685245513916,
          2.938915967941284,
          2.7683141231536865,
          2.72959303855896,
          2.751657009124756,
          2.635035991668701,
          2.952603816986084,
          2.5926730632781982,
          2.743978500366211,
          2.6748952865600586,
          2.6321699619293213,
          2.599555015563965,
          2.7209503650665283,
          2.411038875579834,
          2.499849557876587,
          2.577751398086548,
          2.6193313598632812,
          2.486454486846924,
          2.4765961170196533,
          2.4730727672576904,
          2.432880163192749,
          2.6570885181427,
          2.638134479522705,
          2.553917169570923,
          2.714312791824341,
          2.716587781906128,
          2.773759603500366,
          2.822941541671753,
          2.6827287673950195
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines",
         "name": "Training Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x2",
         "y": [
          0.028925620019435883,
          0.04958677664399147,
          0.056473828852176666,
          0.06749311089515686,
          0.10055096447467804,
          0.12258952856063843,
          0.1542699784040451,
          0.1680440753698349,
          0.19834710657596588,
          0.24517905712127686,
          0.23966942727565765,
          0.294765830039978,
          0.3140496015548706,
          0.3622589409351349,
          0.3471074402332306,
          0.40220385789871216,
          0.42424243688583374,
          0.4573002755641937,
          0.4848484992980957,
          0.5068870782852173,
          0.5619834661483765,
          0.5881542563438416,
          0.6225895285606384,
          0.6280992031097412,
          0.6308540105819702,
          0.7066115736961365,
          0.6818181872367859,
          0.725895345211029,
          0.7327823638916016,
          0.7272727489471436,
          0.7575757503509521,
          0.7396694421768188,
          0.7534435391426086,
          0.7768595218658447,
          0.8099173307418823
         ],
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "lightblue"
         },
         "mode": "lines",
         "name": "Validation Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x2",
         "y": [
          0.04651162773370743,
          0.023255813866853714,
          0.09302325546741486,
          0.11627907305955887,
          0.13953489065170288,
          0.11627907305955887,
          0.20930232107639313,
          0.13953489065170288,
          0.27906978130340576,
          0.27906978130340576,
          0.23255814611911774,
          0.3255814015865326,
          0.27906978130340576,
          0.3255814015865326,
          0.3720930218696594,
          0.3720930218696594,
          0.3255814015865326,
          0.3720930218696594,
          0.25581395626068115,
          0.41860464215278625,
          0.3255814015865326,
          0.3488371968269348,
          0.3488371968269348,
          0.4883720874786377,
          0.3255814015865326,
          0.39534884691238403,
          0.5116279125213623,
          0.4883720874786377,
          0.4651162922382355,
          0.4651162922382355,
          0.41860464215278625,
          0.3720930218696594,
          0.4883720874786377,
          0.41860464215278625,
          0.4651162922382355
         ],
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines",
         "name": "Train Top-3",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x3",
         "y": [
          0.07988980412483215,
          0.10330578684806824,
          0.1584022045135498,
          0.1790633648633957,
          0.23278236389160156,
          0.27410468459129333,
          0.3388429880142212,
          0.3539944887161255,
          0.42148759961128235,
          0.497245192527771,
          0.4944903552532196,
          0.5785123705863953,
          0.5922865271568298,
          0.647382915019989,
          0.6680440902709961,
          0.7066115736961365,
          0.7231404781341553,
          0.7396694421768188,
          0.78925621509552,
          0.8071625232696533,
          0.8471074104309082,
          0.8484848737716675,
          0.8870523571968079,
          0.8856749534606934,
          0.9035812616348267,
          0.9269972443580627,
          0.9283746480941772,
          0.9490358233451843,
          0.9311294555664062,
          0.9696969985961914,
          0.9559228420257568,
          0.9559228420257568,
          0.9600551128387451,
          0.9669421315193176,
          0.9683195352554321
         ],
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "lightgreen"
         },
         "mode": "lines",
         "name": "Val Top-3",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35
         ],
         "xaxis": "x3",
         "y": [
          0.11627907305955887,
          0.20930232107639313,
          0.1627907007932663,
          0.23255814611911774,
          0.27906978130340576,
          0.27906978130340576,
          0.44186046719551086,
          0.39534884691238403,
          0.44186046719551086,
          0.41860464215278625,
          0.5116279125213623,
          0.4651162922382355,
          0.44186046719551086,
          0.5581395626068115,
          0.5581395626068115,
          0.4883720874786377,
          0.604651153087616,
          0.5813953280448914,
          0.604651153087616,
          0.7209302186965942,
          0.6744186282157898,
          0.604651153087616,
          0.6511628031730652,
          0.6511628031730652,
          0.6976743936538696,
          0.6976743936538696,
          0.7209302186965942,
          0.7209302186965942,
          0.7441860437393188,
          0.6976743936538696,
          0.6744186282157898,
          0.6511628031730652,
          0.7209302186965942,
          0.6511628031730652,
          0.6976743936538696
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Training & Validation Loss",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Training & Validation Accuracy",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Top-3 Accuracy",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.45,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Learning Rate",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.45,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Quran Verse Detection - Training Progress"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Epochs"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Epochs"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Epochs"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Loss"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Top-3 Accuracy"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Learning Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model saved successfully!\n",
      "ğŸ“ Location: model_saves_quran_model_final/\n",
      "ğŸ¯ Ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# ======= MAIN EXECUTION: TRAIN MODEL =======\n",
    "\n",
    "if len(X) > 0:\n",
    "    print(\"ğŸ¯ TRAINING QURAN VERSE DETECTION MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, training_history, trained_encoder = train_quran_model(\n",
    "        X, y, model_name=\"quran_model_final\", save_model=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Plot training results\n",
    "    print(\"ğŸ“Š Generating training plots...\")\n",
    "    training_plot = plot_training_results(training_history)\n",
    "    \n",
    "    print(f\"\\nâœ… Model saved successfully!\")\n",
    "    print(f\"ğŸ“ Location: model_saves_quran_model_final/\")\n",
    "    print(f\"ğŸ¯ Ready for testing!\")\n",
    "else:\n",
    "    print(\"âŒ Cannot train: No data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75497dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª LOADING TRAINED MODEL FOR TESTING\n",
      "==================================================\n",
      "âœ… Using model from current training session\n",
      "âœ… Model ready for testing!\n",
      "ğŸ¯ Model can detect 41 different verses\n"
     ]
    }
   ],
   "source": [
    "# ======= TESTING: LOAD TRAINED MODEL =======\n",
    "\n",
    "print(\"ğŸ§ª LOADING TRAINED MODEL FOR TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Try to use model from current session first\n",
    "if 'trained_model' in locals() and 'trained_encoder' in locals():\n",
    "    print(\"âœ… Using model from current training session\")\n",
    "    test_model = trained_model\n",
    "    test_encoder = trained_encoder\n",
    "else:\n",
    "    print(\"ğŸ”„ Loading saved model...\")\n",
    "    test_model, test_encoder, test_metadata = load_trained_model()\n",
    "\n",
    "if test_model is not None:\n",
    "    print(f\"âœ… Model ready for testing!\")\n",
    "    print(f\"ğŸ¯ Model can detect {len(test_encoder.classes_)} different verses\")\n",
    "else:\n",
    "    print(\"âŒ No trained model available\")\n",
    "    print(\"ğŸ’¡ Please train the model first using the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ad75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸµ TESTING SINGLE AUDIO FILE\n",
      "==================================================\n",
      "ğŸµ Testing: test.mp3\n",
      "==================================================\n",
      "ğŸ“ Prediction: Ayat 23\n",
      "ğŸ“Š Confidence: 0.474 (47.4%)\n",
      "âš ï¸  Medium Confidence\n",
      "\n",
      "ğŸ¥‡ Top 3 predictions:\n",
      "   1. Ayat 23: 0.474\n",
      "   2. Ayat 3: 0.331\n",
      "   3. Ayat 6: 0.187\n",
      "ğŸ“ Prediction: Ayat 23\n",
      "ğŸ“Š Confidence: 0.474 (47.4%)\n",
      "âš ï¸  Medium Confidence\n",
      "\n",
      "ğŸ¥‡ Top 3 predictions:\n",
      "   1. Ayat 23: 0.474\n",
      "   2. Ayat 3: 0.331\n",
      "   3. Ayat 6: 0.187\n"
     ]
    }
   ],
   "source": [
    "# ======= TESTING: SINGLE FILE TEST =======\n",
    "\n",
    "if test_model is not None:\n",
    "    print(\"ğŸµ TESTING SINGLE AUDIO FILE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test files to try (in order of preference)\n",
    "    test_files = [\n",
    "        \"test.mp3\",  # User's test file\n",
    "        r\"d:\\new_project\\quran_detect\\test.mp3\",\n",
    "        r\"d:\\new_project\\quran_detect\\sample_1\\078000.mp3\",  # Bismillah\n",
    "        r\"d:\\new_project\\quran_detect\\sample_1\\078001.mp3\",  # Ayat 1\n",
    "        r\"d:\\new_project\\quran_detect\\sample_1\\078005.mp3\",  # Ayat 5\n",
    "    ]\n",
    "    \n",
    "    test_found = False\n",
    "    for test_file in test_files:\n",
    "        if os.path.exists(test_file):\n",
    "            test_audio_file(test_model, test_encoder, test_file)\n",
    "            test_found = True\n",
    "            break\n",
    "    \n",
    "    if not test_found:\n",
    "        print(\"âŒ No test files found. Please:\")\n",
    "        print(\"   1. Place test.mp3 in the project folder, or\")\n",
    "        print(\"   2. Ensure sample folders contain audio files\")\n",
    "        print(\"\\nğŸ’¡ You can test any audio file by calling:\")\n",
    "        print(\"   test_audio_file(test_model, test_encoder, 'path/to/your/audio.mp3')\")\n",
    "else:\n",
    "    print(\"âŒ No model available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec2bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ BATCH PERFORMANCE TESTING\n",
      "==================================================\n",
      "\n",
      "ğŸ“‚ Testing folder: sample_1\n",
      "ğŸ§ª Testing 10 files from sample_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=792\n",
      "\n",
      "Testing:  10%|â–ˆ         | 1/10 [00:00<00:01,  6.27it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=792\n",
      "\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.39it/s]\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "   Accuracy: 70.0% (7/10)\n",
      "   Average confidence: 0.586\n",
      "\n",
      "ğŸ“‹ Sample results:\n",
      "   âŒ Bismillah (Pembuka) -> Ayat 25 (0.70)\n",
      "   âœ… Ayat 1 -> Ayat 1 (0.21)\n",
      "   âœ… Ayat 2 -> Ayat 2 (0.50)\n",
      "   âœ… Ayat 3 -> Ayat 3 (0.93)\n",
      "   âœ… Ayat 4 -> Ayat 4 (0.66)\n",
      "\n",
      "ğŸ“‚ Testing folder: sample_2\n",
      "ğŸ§ª Testing 10 files from sample_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.60it/s]\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "   Accuracy: 80.0% (8/10)\n",
      "   Average confidence: 0.677\n",
      "\n",
      "ğŸ“‹ Sample results:\n",
      "   âœ… Bismillah (Pembuka) -> Bismillah (Pembuka) (0.82)\n",
      "   âŒ Ayat 1 -> Ayat 2 (0.52)\n",
      "   âœ… Ayat 2 -> Ayat 2 (0.52)\n",
      "   âœ… Ayat 3 -> Ayat 3 (0.95)\n",
      "   âœ… Ayat 4 -> Ayat 4 (0.75)\n",
      "\n",
      "ğŸ“‚ Testing folder: sample_3\n",
      "ğŸ§ª Testing 10 files from sample_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=984\n",
      "\n",
      "Testing:  10%|â–ˆ         | 1/10 [00:00<00:01,  6.54it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=984\n",
      "\n",
      "Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:01<00:00,  6.01it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=944\n",
      "\n",
      "Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:01<00:00,  6.14it/s]c:\\Users\\User\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning:\n",
      "\n",
      "n_fft=1024 is too large for input signal of length=944\n",
      "\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "   Accuracy: 80.0% (8/10)\n",
      "   Average confidence: 0.690\n",
      "\n",
      "ğŸ“‹ Sample results:\n",
      "   âœ… Bismillah (Pembuka) -> Bismillah (Pembuka) (0.84)\n",
      "   âŒ Ayat 1 -> Ayat 2 (0.51)\n",
      "   âœ… Ayat 2 -> Ayat 2 (0.56)\n",
      "   âœ… Ayat 3 -> Ayat 3 (0.93)\n",
      "   âœ… Ayat 4 -> Ayat 4 (0.68)\n",
      "\n",
      "ğŸ¯ OVERALL PERFORMANCE:\n",
      "   Total files tested: 30\n",
      "   Overall accuracy: 76.7%\n",
      "   Average confidence: 0.651\n",
      "   Bismillah accuracy: 66.7%\n",
      "   Verses accuracy: 77.8%\n",
      "\n",
      "ğŸ‰ Testing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======= TESTING: BATCH PERFORMANCE TEST =======\n",
    "\n",
    "if test_model is not None:\n",
    "    print(\"ğŸ”¬ BATCH PERFORMANCE TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test on sample folders\n",
    "    test_folders = [\n",
    "        r\"d:\\new_project\\quran_detect\\sample_1\",\n",
    "        r\"d:\\new_project\\quran_detect\\sample_2\",\n",
    "        r\"d:\\new_project\\quran_detect\\sample_3\"\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for folder in test_folders:\n",
    "        if os.path.exists(folder):\n",
    "            print(f\"\\nğŸ“‚ Testing folder: {os.path.basename(folder)}\")\n",
    "            results = test_folder_performance(test_model, test_encoder, folder, max_files=10)\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    if all_results:\n",
    "        # Overall statistics\n",
    "        total_correct = sum([r['correct'] for r in all_results])\n",
    "        total_tested = len(all_results)\n",
    "        overall_accuracy = total_correct / total_tested\n",
    "        overall_confidence = np.mean([r['confidence'] for r in all_results])\n",
    "        \n",
    "        print(f\"\\nğŸ¯ OVERALL PERFORMANCE:\")\n",
    "        print(f\"   Total files tested: {total_tested}\")\n",
    "        print(f\"   Overall accuracy: {overall_accuracy:.1%}\")\n",
    "        print(f\"   Average confidence: {overall_confidence:.3f}\")\n",
    "        \n",
    "        # Performance by verse type\n",
    "        bismillah_results = [r for r in all_results if r['actual'] == 0]\n",
    "        verse_results = [r for r in all_results if r['actual'] > 0]\n",
    "        \n",
    "        if bismillah_results:\n",
    "            bismillah_acc = sum([r['correct'] for r in bismillah_results]) / len(bismillah_results)\n",
    "            print(f\"   Bismillah accuracy: {bismillah_acc:.1%}\")\n",
    "        \n",
    "        if verse_results:\n",
    "            verse_acc = sum([r['correct'] for r in verse_results]) / len(verse_results)\n",
    "            print(f\"   Verses accuracy: {verse_acc:.1%}\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Testing completed!\")\n",
    "    else:\n",
    "        print(\"âŒ No test results obtained\")\n",
    "else:\n",
    "    print(\"âŒ No model available for batch testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cd154",
   "metadata": {},
   "source": [
    "# ğŸ¯ Usage Instructions\n",
    "\n",
    "## For Training:\n",
    "1. **Run all cells in order** - The notebook will automatically:\n",
    "   - Load and preprocess audio data\n",
    "   - Train the model with advanced features\n",
    "   - Save the model and metadata\n",
    "   - Generate training visualizations\n",
    "\n",
    "## For Testing:\n",
    "1. **Place test.mp3** in the project folder\n",
    "2. **Run testing cells** - The notebook will:\n",
    "   - Load the trained model\n",
    "   - Test your audio file\n",
    "   - Show prediction with confidence\n",
    "   - Display top-3 predictions\n",
    "\n",
    "## Custom Testing:\n",
    "```python\n",
    "# Test any audio file\n",
    "test_audio_file(test_model, test_encoder, 'path/to/your/audio.mp3')\n",
    "\n",
    "# Test performance on folder\n",
    "test_folder_performance(test_model, test_encoder, 'path/to/folder')\n",
    "```\n",
    "\n",
    "## Expected Performance:\n",
    "- **Accuracy**: 80-90%+\n",
    "- **Top-3 Accuracy**: 90-95%+\n",
    "- **Features**: 80+ advanced audio features\n",
    "- **Classes**: 41 (Bismillah + 40 Ayat)\n",
    "\n",
    "## Files Generated:\n",
    "```\n",
    "model_saves_quran_model_final/\n",
    "â”œâ”€â”€ quran_model.h5          # Trained TensorFlow model\n",
    "â”œâ”€â”€ label_encoder.pkl       # Label encoder for predictions\n",
    "â”œâ”€â”€ metadata.json          # Model info and performance\n",
    "â”œâ”€â”€ training_history.pkl   # Training history\n",
    "â””â”€â”€ best_model.h5         # Best checkpoint\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
